#!/usr/bin/env python3
"""
WSINDy ROM Pipeline — PDE Discovery for Particle Density Fields
================================================================

Discovers a PDE governing the evolution of particle density fields
generated by the Vicsek/Morse simulation.  Then uses the discovered PDE
to forecast test density evolution, producing artifacts compatible
with the MVAR/LSTM ROM pipeline for direct comparison.

Pipeline:
  1. Load experiment (config, training densities, test densities)
  2. Build library (polynomial features × spatial operators)
  3. Multi-trajectory PDE discovery (stacked WSINDy weak system)
  4. Bootstrap uncertainty quantification
  5. Evaluate on test runs (forecast + time-resolved metrics)
  6. Save artifacts  (density_pred_wsindy.npz, r2_vs_time, test_results.csv)
  7. Generate MVAR vs WSINDy comparison plots

Usage:
  python run_wsindy_rom.py --experiment DYN1_gentle_v2
  python run_wsindy_rom.py --experiment DYN1_gentle_v2 --n_train 10
"""

import numpy as np
import json
import time
import argparse
import yaml
import sys
from pathlib import Path
import pandas as pd

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

sys.path.insert(0, str(Path(__file__).parent / "src"))

from wsindy.grid import GridSpec
from wsindy.system import (
    build_weak_system,
    default_t_margin,
    make_query_indices,
)
from wsindy.test_functions import make_separable_psi
from wsindy.model import WSINDyModel
from wsindy.fit import wsindy_fit_regression
from wsindy.integrators import split_linear_nonlinear
from wsindy.forecast import wsindy_forecast
from wsindy.pretty import to_text, to_latex, group_terms
from wsindy.library import default_library
from wsindy.select import (
    TrialResult,
    SelectionResult,
    _pareto_frontier,
    _composite_score,
    default_ell_grid,
)


# ═══════════════════════════════════════════════════════════════════
#  Data Loading
# ═══════════════════════════════════════════════════════════════════

def load_density(run_dir, filename="density.npz"):
    """Load density trajectory from a run directory."""
    d = np.load(Path(run_dir) / filename)
    return {
        "rho": d["rho"],
        "xgrid": d["xgrid"],
        "ygrid": d["ygrid"],
        "times": d["times"],
    }


def subsample_density(data, factor=3):
    """Subsample density trajectory in time."""
    return {
        "rho": data["rho"][::factor],
        "xgrid": data["xgrid"],
        "ygrid": data["ygrid"],
        "times": data["times"][::factor],
    }


def grid_from_density(data):
    """Create WSINDy GridSpec from density data."""
    dx = float(data["xgrid"][1] - data["xgrid"][0])
    dy = float(data["ygrid"][1] - data["ygrid"][0])
    dt = float(data["times"][1] - data["times"][0])
    return GridSpec(dt=dt, dx=dx, dy=dy)


def select_training_runs(train_dir, metadata, n_runs=10, seed=42):
    """Select a diverse subset of training runs (stratified by IC type)."""
    rng = np.random.default_rng(seed)

    by_type = {}
    for m in metadata:
        ic = m.get("distribution", m.get("ic_type", "unknown"))
        by_type.setdefault(ic, []).append(m)

    types = sorted(by_type.keys())
    per_type = max(1, n_runs // len(types))
    remainder = n_runs - per_type * len(types)

    selected = []
    for i, ic_type in enumerate(types):
        runs = by_type[ic_type]
        n = per_type + (1 if i < remainder else 0)
        n = min(n, len(runs))
        idx = rng.choice(len(runs), size=n, replace=False)
        for j in idx:
            selected.append(runs[j])

    return selected[:n_runs]


# ═══════════════════════════════════════════════════════════════════
#  Multi-trajectory WSINDy
# ═══════════════════════════════════════════════════════════════════

def build_stacked_weak_system(
    train_densities, grid, psi_bundle, library_terms, stride=(2, 2, 2),
):
    """Build stacked weak system (b, G) from multiple training trajectories.

    For each trajectory U_k, builds (b_k, G_k) at query points with the
    given test functions, then stacks them vertically.

    Returns
    -------
    b : (N_total,) stacked LHS
    G : (N_total, M) stacked RHS
    col_names : list[str]
    """
    all_b, all_G = [], []
    t_margin = default_t_margin(psi_bundle)

    for U_k in train_densities:
        T_k, nx, ny = U_k.shape
        if 2 * t_margin >= T_k:
            continue
        qi = make_query_indices(
            T_k, nx, ny,
            stride_t=stride[0], stride_x=stride[1], stride_y=stride[2],
            t_margin=t_margin,
        )
        if qi.shape[0] < len(library_terms) + 1:
            continue
        b_k, G_k, col_names = build_weak_system(
            U_k, grid, psi_bundle, library_terms, qi,
        )
        all_b.append(b_k)
        all_G.append(G_k)

    if not all_b:
        raise ValueError("No valid query points from any training trajectory")

    return np.concatenate(all_b), np.vstack(all_G), col_names


def fit_stacked(
    train_densities, grid, library_terms, ell, p,
    stride=(2, 2, 2), lambdas=None,
):
    """Fit WSINDy from multiple trajectories at a given ℓ.

    Returns (model, b, G) so the system can be reused for bootstrap.
    """
    psi_bundle = make_separable_psi(
        grid,
        ellt=ell[0], ellx=ell[1], elly=ell[2],
        pt=p[0], px=p[1], py=p[2],
    )
    b, G, col_names = build_stacked_weak_system(
        train_densities, grid, psi_bundle, library_terms, stride,
    )
    model = wsindy_fit_regression(b, G, col_names, lambdas=lambdas)
    return model, b, G, col_names


def model_selection_stacked(
    train_densities, grid, library_terms, ell_grid, p,
    stride=(2, 2, 2), lambdas=None,
    alpha=0.1, beta=0.01, cond_threshold=1e8,
    verbose=True,
):
    """Model selection over ℓ with multi-trajectory stacking."""
    if lambdas is None:
        lambdas = np.logspace(-4, 1, 40)

    n_library = len(library_terms)
    trials = []

    for idx, ell in enumerate(ell_grid):
        t0 = time.perf_counter()
        try:
            model, b, G, col_names = fit_stacked(
                train_densities, grid, library_terms, ell, p, stride, lambdas,
            )
        except Exception as exc:
            if verbose:
                print(f"  [{idx+1}/{len(ell_grid)}] ℓ={ell} FAILED: {exc}")
            continue

        elapsed = time.perf_counter() - t0
        diag = model.diagnostics
        nloss = diag.get("normalised_loss", float("inf"))
        r2w = diag.get("r2", 0.0)

        # Condition number of active sub-matrix
        active_cols = G[:, model.active]
        cond = float(np.linalg.cond(active_cols)) if active_cols.shape[1] > 0 else float("inf")

        score = _composite_score(
            nloss, model.n_active, n_library, cond, alpha, beta, cond_threshold,
        )

        trial = TrialResult(
            ell=ell, p=p, stride=stride, model=model,
            n_query=b.shape[0],
            normalised_loss=nloss, r2_weak=r2w,
            n_active=model.n_active, best_lambda=model.best_lambda,
            condition_number=cond, composite_score=score, elapsed_s=elapsed,
        )
        trials.append(trial)

        if verbose:
            print(
                f"  [{idx+1}/{len(ell_grid)}] ℓ={ell} s={stride}  "
                f"active={model.n_active}  loss={nloss:.4e}  "
                f"score={score:.4e}  ({elapsed:.2f}s)"
            )

    if not trials:
        raise RuntimeError("All model selection trials failed")

    ranked = sorted(trials, key=lambda t: t.composite_score)
    best = ranked[0]
    pareto = _pareto_frontier(trials)
    result = SelectionResult(trials=trials, best=best, pareto=pareto)

    # Rebuild system for best ℓ (for bootstrap reuse)
    _, best_b, best_G, _ = fit_stacked(
        train_densities, grid, library_terms,
        best.ell, p, stride, lambdas,
    )
    return result, best_b, best_G


def bootstrap_from_system(
    b, G, col_names, B=30, lambdas=None, ci_alpha=0.05, seed=42,
):
    """Bootstrap UQ by resampling rows of the pre-built weak system."""
    rng = np.random.default_rng(seed)
    if lambdas is None:
        lambdas = np.logspace(-4, 1, 40)

    N = b.shape[0]
    M = len(col_names)
    coeff_samples = np.zeros((B, M))
    active_counts = np.zeros(M)

    for rep in range(B):
        idx = rng.choice(N, size=N, replace=True)
        model = wsindy_fit_regression(b[idx], G[idx], col_names, lambdas=lambdas)
        coeff_samples[rep] = model.w
        active_counts += model.active.astype(float)

    inc_prob = active_counts / B
    lo = np.percentile(coeff_samples, 100 * ci_alpha / 2, axis=0)
    hi = np.percentile(coeff_samples, 100 * (1 - ci_alpha / 2), axis=0)

    return {
        "coeff_samples": coeff_samples,
        "coeff_mean": coeff_samples.mean(axis=0),
        "coeff_std": coeff_samples.std(axis=0),
        "inclusion_probability": inc_prob,
        "ci_lo": lo,
        "ci_hi": hi,
        "B": B,
        "col_names": col_names,
    }


# ═══════════════════════════════════════════════════════════════════
#  Forecasting & Evaluation
# ═══════════════════════════════════════════════════════════════════

def forecast_density(model, grid, U0, n_steps, clip_negative=True):
    """Forecast density from IC using discovered PDE. Returns (U_pred, method)."""
    lin, _ = split_linear_nonlinear(model)
    method = "auto"

    try:
        U_pred = wsindy_forecast(
            U0, model, grid, n_steps=n_steps,
            method=method, clip_negative=clip_negative,
        )
    except Exception:
        # Fallback to RK4 if ETDRK4 fails
        U_pred = wsindy_forecast(
            U0, model, grid, n_steps=n_steps,
            method="rk4", clip_negative=clip_negative,
        )
        method = "rk4"

    # Safety: check for NaN / blow-up
    if np.any(np.isnan(U_pred)) or np.max(np.abs(U_pred)) > 1e10:
        raise RuntimeError("Forecast diverged (NaN or blow-up)")

    return U_pred, "etdrk4" if lin and method == "auto" else "rk4"


def compute_r2_timeseries(rho_true, rho_pred):
    T = min(rho_true.shape[0], rho_pred.shape[0])
    r2 = np.zeros(T)
    for t in range(T):
        ss_res = np.sum((rho_true[t] - rho_pred[t]) ** 2)
        ss_tot = np.sum((rho_true[t] - rho_true[t].mean()) ** 2)
        r2[t] = 1.0 - ss_res / max(ss_tot, 1e-30)
    return r2


def compute_rmse_timeseries(rho_true, rho_pred):
    T = min(rho_true.shape[0], rho_pred.shape[0])
    return np.array([np.sqrt(np.mean((rho_true[t] - rho_pred[t])**2)) for t in range(T)])


def compute_rel_l2_timeseries(rho_true, rho_pred):
    T = min(rho_true.shape[0], rho_pred.shape[0])
    return np.array([
        np.sqrt(np.sum((rho_true[t] - rho_pred[t])**2)) / max(np.sqrt(np.sum(rho_true[t]**2)), 1e-30)
        for t in range(T)
    ])


# ═══════════════════════════════════════════════════════════════════
#  Plots
# ═══════════════════════════════════════════════════════════════════

def plot_discovered_pde(model, boot_result, out_path):
    """Discovered PDE coefficients with optional bootstrap CI."""
    active_idx = np.where(model.active)[0]
    names = [model.col_names[i] for i in active_idx]
    coeffs = model.w[active_idx]

    fig, ax = plt.subplots(figsize=(max(8, len(names) * 1.2), 4))
    x = np.arange(len(names))

    if boot_result is not None:
        means = boot_result["coeff_mean"][active_idx]
        lo = boot_result["ci_lo"][active_idx]
        hi = boot_result["ci_hi"][active_idx]
        ax.errorbar(x, means, yerr=[means - lo, hi - means], fmt="o",
                    capsize=6, color="steelblue", markersize=8, linewidth=2,
                    label="Bootstrap mean +/- 95% CI")
    else:
        ax.bar(x, coeffs, color="steelblue", alpha=0.8)

    ax.set_xticks(x)
    ax.set_xticklabels(names, fontsize=11)
    ax.set_ylabel("Coefficient", fontsize=12)
    ax.set_title("Discovered PDE Coefficients (WSINDy)", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3, axis="y")
    ax.axhline(0, color="k", linewidth=0.5)
    if boot_result:
        ax.legend(fontsize=10)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_r2_per_test(wsindy_df, mvar_df, out_path):
    """Side-by-side bar chart of R² per test run."""
    n = len(wsindy_df)
    x = np.arange(n)
    width = 0.35

    fig, ax = plt.subplots(figsize=(max(10, n * 0.5), 5))
    if mvar_df is not None and len(mvar_df) == n:
        ax.bar(x - width / 2, mvar_df["r2_reconstructed"], width,
               label="MVAR", alpha=0.8, color="steelblue")
    ax.bar(x + width / 2, wsindy_df["r2_reconstructed"], width,
           label="WSINDy", alpha=0.8, color="darkorange")

    ax.set_xlabel("Test Run", fontsize=12)
    ax.set_ylabel("R²", fontsize=12)
    ax.set_title("MVAR vs WSINDy: Per-Test R²", fontsize=14, fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels([str(i) for i in range(n)], fontsize=8)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis="y")
    ax.axhline(0, color="red", linestyle="--", alpha=0.3)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_mean_r2_vs_time(wsindy_r2_mat, mvar_r2_mat, times, out_path):
    """Mean R² ± std over time across all test runs."""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), height_ratios=[3, 1])

    w_mean = wsindy_r2_mat.mean(axis=0)
    w_std = wsindy_r2_mat.std(axis=0)
    ax1.plot(times, w_mean, "-", color="darkorange", lw=2.5,
             label=f"WSINDy (mean R²={w_mean.mean():.3f})", alpha=0.9)
    ax1.fill_between(times, w_mean - w_std, w_mean + w_std,
                     color="darkorange", alpha=0.15)

    if mvar_r2_mat is not None:
        m_mean = mvar_r2_mat.mean(axis=0)
        m_std = mvar_r2_mat.std(axis=0)
        ax1.plot(times, m_mean, "-", color="steelblue", lw=2.5,
                 label=f"MVAR (mean R²={m_mean.mean():.3f})", alpha=0.9)
        ax1.fill_between(times, m_mean - m_std, m_mean + m_std,
                         color="steelblue", alpha=0.15)

    ax1.set_ylabel("R²", fontsize=13)
    ax1.set_title("Mean R² Over Time (All Test Runs)", fontsize=14, fontweight="bold")
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(-0.1, 1.05)
    ax1.axhline(0, color="red", linestyle="--", alpha=0.3)

    if mvar_r2_mat is not None:
        diff = w_mean - m_mean
        ax2.fill_between(times, 0, diff, where=diff >= 0,
                         color="darkorange", alpha=0.4, label="WSINDy > MVAR")
        ax2.fill_between(times, 0, diff, where=diff < 0,
                         color="steelblue", alpha=0.4, label="MVAR > WSINDy")
        ax2.plot(times, diff, "k-", linewidth=1, alpha=0.5)
        ax2.axhline(0, color="k", linewidth=1)
        ax2.set_ylabel("Delta R²", fontsize=11)
        ax2.legend(fontsize=9, ncol=2)
        ax2.grid(True, alpha=0.3)
    else:
        ax2.set_visible(False)

    ax2.set_xlabel("Time (s)", fontsize=13)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_density_snapshots(rho_true, rho_wsindy, rho_mvar,
                           snap_indices, times, out_path):
    """True vs WSINDy vs MVAR density snapshots."""
    n = len(snap_indices)
    has_mvar = rho_mvar is not None
    nrows = 4 if has_mvar else 3  # True / WSINDy / MVAR / |Error|

    fig, axes = plt.subplots(nrows, n, figsize=(4 * n, 3.3 * nrows))
    if n == 1:
        axes = axes[:, np.newaxis]

    vmin = min(rho_true.min(), rho_wsindy.min())
    vmax = max(rho_true.max(), rho_wsindy.max())

    for j, ti in enumerate(snap_indices):
        t_val = times[ti] if ti < len(times) else 0

        axes[0, j].imshow(rho_true[ti].T, origin="lower", cmap="hot",
                          vmin=vmin, vmax=vmax, aspect="equal")
        axes[0, j].set_title(f"True  t={t_val:.1f}s", fontsize=10)
        axes[0, j].axis("off")

        axes[1, j].imshow(rho_wsindy[ti].T, origin="lower", cmap="hot",
                          vmin=vmin, vmax=vmax, aspect="equal")
        axes[1, j].set_title(f"WSINDy  t={t_val:.1f}s", fontsize=10)
        axes[1, j].axis("off")

        if has_mvar:
            axes[2, j].imshow(rho_mvar[ti].T, origin="lower", cmap="hot",
                              vmin=vmin, vmax=vmax, aspect="equal")
            axes[2, j].set_title(f"MVAR  t={t_val:.1f}s", fontsize=10)
            axes[2, j].axis("off")

        err_row = 3 if has_mvar else 2
        diff_w = np.abs(rho_true[ti] - rho_wsindy[ti])
        axes[err_row, j].imshow(diff_w.T, origin="lower", cmap="Reds", aspect="equal")
        axes[err_row, j].set_title(f"|Error WSINDy| t={t_val:.1f}s", fontsize=9)
        axes[err_row, j].axis("off")

    fig.suptitle("Density Field Comparison", fontsize=14, fontweight="bold", y=1.01)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


def plot_summary_stats(wsindy_df, mvar_df, model, out_path):
    """Summary statistics panel: violin + text."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    data_to_plot = [wsindy_df["r2_reconstructed"].dropna().values]
    labels = ["WSINDy"]
    if mvar_df is not None:
        data_to_plot.append(mvar_df["r2_reconstructed"].dropna().values)
        labels.append("MVAR")

    positions = list(range(1, len(data_to_plot) + 1))
    parts = ax1.violinplot(data_to_plot, positions=positions, showmeans=True, showmedians=True)
    ax1.set_xticks(positions)
    ax1.set_xticklabels(labels)
    ax1.set_ylabel("R²", fontsize=12)
    ax1.set_title("R² Distribution (All Tests)", fontsize=13, fontweight="bold")
    ax1.grid(True, alpha=0.3, axis="y")
    ax1.axhline(0, color="red", linestyle="--", alpha=0.4)

    # Summary text
    ax2.axis("off")
    w_mean = wsindy_df["r2_reconstructed"].mean()
    w_std = wsindy_df["r2_reconstructed"].std()
    txt = f"WSINDy Performance:\n"
    txt += f"  Mean R²:  {w_mean:.4f} +/- {w_std:.4f}\n"
    txt += f"  N tests:  {len(wsindy_df)}\n\n"

    if mvar_df is not None:
        m_mean = mvar_df["r2_reconstructed"].mean()
        m_std = mvar_df["r2_reconstructed"].std()
        diff = w_mean - m_mean
        txt += f"MVAR Performance:\n"
        txt += f"  Mean R²:  {m_mean:.4f} +/- {m_std:.4f}\n"
        txt += f"  N tests:  {len(mvar_df)}\n\n"
        txt += f"Comparison:\n"
        txt += f"  Delta R²: {diff:+.4f}\n"
        txt += f"  Winner:   {'WSINDy' if diff > 0 else 'MVAR'}\n\n"

    txt += f"Discovered PDE:\n"
    txt += f"  {to_text(model)}\n"
    txt += f"  Active terms: {model.n_active}\n"

    ax2.text(0.05, 0.95, txt, fontsize=11, family="monospace",
             verticalalignment="top", transform=ax2.transAxes,
             bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))

    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


# ═══════════════════════════════════════════════════════════════════
#  Main Pipeline
# ═══════════════════════════════════════════════════════════════════

def main():
    parser = argparse.ArgumentParser(
        description="WSINDy ROM Pipeline — PDE Discovery for Density Fields")
    parser.add_argument("--experiment", type=str, required=True,
                        help="Experiment name (e.g., DYN1_gentle_v2)")
    parser.add_argument("--data_dir", type=str, default="oscar_output",
                        help="Base data directory (default: oscar_output)")
    parser.add_argument("--n_train", type=int, default=10,
                        help="Number of training runs for PDE discovery")
    parser.add_argument("--subsample", type=int, default=None,
                        help="Temporal subsample factor (default: from config)")
    parser.add_argument("--max_poly", type=int, default=2,
                        help="Max polynomial degree in library")
    parser.add_argument("--operators", nargs="+",
                        default=["I", "dx", "dy", "lap"],
                        help="Spatial operators for library")
    parser.add_argument("--n_ell", type=int, default=8,
                        help="Number of test-function scale configs")
    parser.add_argument("--n_bootstrap", type=int, default=30,
                        help="Bootstrap replicates (0 to disable)")
    parser.add_argument("--no_clip", action="store_true",
                        help="Do NOT clip negative densities in forecast")
    args = parser.parse_args()

    clip_neg = not args.no_clip
    t_pipeline_start = time.perf_counter()
    sep = "=" * 70
    thin = "-" * 70

    # ================================================================
    # Setup
    # ================================================================
    EXP_DIR = Path(args.data_dir) / args.experiment
    TRAIN_DIR = EXP_DIR / "train"
    TEST_DIR = EXP_DIR / "test"
    WSINDY_DIR = EXP_DIR / "WSINDy"
    PLOTS_DIR = WSINDY_DIR / "plots"

    WSINDY_DIR.mkdir(parents=True, exist_ok=True)
    PLOTS_DIR.mkdir(parents=True, exist_ok=True)

    print(f"\n{sep}")
    print("  WSINDy ROM PIPELINE — PDE Discovery for Density Fields")
    print(f"{sep}")
    print(f"  Experiment : {args.experiment}")
    print(f"  Data       : {EXP_DIR}")
    print(f"  Output     : {WSINDY_DIR}")
    print(f"  Timestamp  : {time.strftime('%Y-%m-%d %H:%M:%S')}")

    if not EXP_DIR.exists():
        print(f"\n  ERROR: {EXP_DIR} not found"); return
    if not TRAIN_DIR.exists():
        print(f"\n  ERROR: {TRAIN_DIR} not found"); return

    # Load experiment config
    config_path = EXP_DIR / "config_used.yaml"
    config = {}
    if config_path.exists():
        with open(config_path) as f:
            config = yaml.safe_load(f)

    rom_cfg = config.get("rom", {})
    subsample = args.subsample or rom_cfg.get("subsample", 3)
    mvar_lag = rom_cfg.get("models", {}).get("mvar", {}).get("lag", 5)

    print(f"  Subsample  : {subsample}")
    print(f"  MVAR lag   : {mvar_lag}  (WSINDy forecast starts at same point)")

    # ================================================================
    # STEP 1: Load training data
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 1: Loading training data")
    print(f"{thin}")

    meta_path = TRAIN_DIR / "metadata.json"
    if meta_path.exists():
        with open(meta_path) as f:
            train_metadata = json.load(f)
    else:
        train_dirs = sorted(d.name for d in TRAIN_DIR.iterdir()
                            if d.is_dir() and d.name.startswith("train_"))
        train_metadata = [{"run_name": d, "distribution": "unknown"} for d in train_dirs]

    selected = select_training_runs(TRAIN_DIR, train_metadata, n_runs=args.n_train)
    print(f"  Total training runs: {len(train_metadata)}")
    print(f"  Requested for WSINDy: {len(selected)}")

    # Only keep runs that actually have density.npz
    available = [m for m in selected
                 if (TRAIN_DIR / m["run_name"] / "density.npz").exists()]
    if not available:
        # Fall back: scan ALL training dirs for density files
        available = [m for m in train_metadata
                     if (TRAIN_DIR / m["run_name"] / "density.npz").exists()]
    if not available:
        print("\n  ERROR: No training runs have density.npz files")
        return
    if len(available) < len(selected):
        print(f"  WARNING: Only {len(available)} runs have density.npz "
              f"(requested {len(selected)})")
    selected = available
    print(f"  Using for WSINDy: {len(selected)}")

    train_densities = []
    for meta in selected:
        run_name = meta["run_name"]
        data = load_density(TRAIN_DIR / run_name, "density.npz")
        data_sub = subsample_density(data, subsample)
        train_densities.append(data_sub["rho"])
        ic_type = meta.get("distribution", meta.get("ic_type", "?"))
        print(f"    {run_name}: {data_sub['rho'].shape} ({ic_type})")

    ref_data = load_density(TRAIN_DIR / selected[0]["run_name"], "density.npz")
    ref_data = subsample_density(ref_data, subsample)
    grid = grid_from_density(ref_data)

    T_sub, nx, ny = train_densities[0].shape
    print(f"\n  Grid: dt={grid.dt:.4f}, dx={grid.dx:.4f}, dy={grid.dy:.4f}")
    print(f"  Shape per run: ({T_sub}, {nx}, {ny})")

    # ================================================================
    # STEP 2: Build library
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 2: Building library")
    print(f"{thin}")

    library_terms = default_library(
        max_poly=args.max_poly, operators=args.operators,
    )
    print(f"  Max poly   : {args.max_poly}")
    print(f"  Operators  : {args.operators}")
    print(f"  Library    : {len(library_terms)} terms")
    for op, feat in library_terms:
        print(f"    {op}:{feat}")

    # ================================================================
    # STEP 3: Model selection (multi-trajectory)
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 3: Multi-trajectory model selection")
    print(f"{thin}")

    ell_grid = default_ell_grid(T_sub, nx, ny, n_points=args.n_ell)
    p = (2, 2, 2)

    print(f"  ell grid   : {len(ell_grid)} configs")
    print(f"  p          : {p}")
    print(f"  Trajectories: {len(train_densities)}")
    print()

    t0 = time.perf_counter()
    sel_result, best_b, best_G = model_selection_stacked(
        train_densities, grid, library_terms, ell_grid, p,
        alpha=0.1, beta=0.01,
    )
    ms_time = time.perf_counter() - t0

    model = sel_result.best.model
    col_names = model.col_names

    print(f"\n  Completed in {ms_time:.1f}s")
    print(sel_result.summary())
    print(f"\n  Discovered PDE:")
    print(f"    {to_text(model)}")
    print(f"\n  LaTeX:")
    print(f"    {to_latex(model)}")
    print(f"\n  Term groups:")
    for group, entries in group_terms(model).items():
        terms_str = ", ".join(f"{e['term']}({e['coeff']:+.3e})" for e in entries)
        print(f"    {group:12s}: {terms_str}")

    # ================================================================
    # STEP 4: Bootstrap UQ
    # ================================================================
    boot_result = None
    if args.n_bootstrap > 0:
        print(f"\n{thin}")
        print("  STEP 4: Bootstrap UQ")
        print(f"{thin}")

        t0 = time.perf_counter()
        boot_result = bootstrap_from_system(
            best_b, best_G, col_names, B=args.n_bootstrap,
        )
        boot_time = time.perf_counter() - t0

        print(f"  {args.n_bootstrap} replicates in {boot_time:.1f}s")
        print(f"\n  {'Term':>12s}  {'Mean':>12s}  {'Std':>12s}  {'P(active)':>10s}")
        for i, name in enumerate(col_names):
            if boot_result["inclusion_probability"][i] > 0.01:
                print(f"  {name:>12s}  {boot_result['coeff_mean'][i]:+12.4e}  "
                      f"{boot_result['coeff_std'][i]:12.4e}  "
                      f"{boot_result['inclusion_probability'][i]:10.3f}")

    # ================================================================
    # STEP 5: Test evaluation
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 5: Evaluating on test runs")
    print(f"{thin}")

    test_meta_path = TEST_DIR / "metadata.json"
    with open(test_meta_path) as f:
        test_metadata = json.load(f)
    n_test = len(test_metadata)
    print(f"  Test runs: {n_test}")

    # Load MVAR results for comparison
    mvar_results_path = EXP_DIR / "MVAR" / "test_results.csv"
    mvar_df = None
    if mvar_results_path.exists():
        mvar_df = pd.read_csv(mvar_results_path)
        print(f"  MVAR results loaded (mean R²={mvar_df['r2_reconstructed'].mean():.4f})")

    test_results = []
    all_wsindy_r2 = []
    all_mvar_r2 = []
    common_times = None

    for test_idx in range(n_test):
        run_name = test_metadata[test_idx]["run_name"]
        test_run_dir = TEST_DIR / run_name

        # Load & subsample test density
        test_data = load_density(test_run_dir, "density_true.npz")
        test_sub = subsample_density(test_data, subsample)
        rho_true_sub = test_sub["rho"]
        times_sub = test_sub["times"]
        T_test_sub = rho_true_sub.shape[0]

        # Forecast starts after MVAR lag (for fair comparison)
        forecast_start = mvar_lag
        n_forecast = T_test_sub - forecast_start - 1

        if n_forecast <= 0:
            print(f"  [{test_idx+1}/{n_test}] {run_name}: SKIP")
            continue

        U0 = rho_true_sub[forecast_start]

        try:
            U_pred, method = forecast_density(
                model, grid, U0, n_forecast, clip_negative=clip_neg,
            )
        except Exception as e:
            print(f"  [{test_idx+1}/{n_test}] {run_name}: FORECAST FAILED - {e}")
            test_results.append({
                "test_id": test_idx, "run_name": run_name,
                "r2_reconstructed": float("nan"), "rmse_recon": float("nan"),
            })
            continue

        # Slice true data to match forecast window
        rho_true_fc = rho_true_sub[forecast_start:forecast_start + n_forecast + 1]
        times_fc = times_sub[forecast_start:forecast_start + n_forecast + 1]

        # Metrics
        r2_ts = compute_r2_timeseries(rho_true_fc, U_pred)
        rmse_ts = compute_rmse_timeseries(rho_true_fc, U_pred)
        rel_l2_ts = compute_rel_l2_timeseries(rho_true_fc, U_pred)

        mean_r2 = float(np.nanmean(r2_ts[1:]))      # skip IC
        mean_rmse = float(np.nanmean(rmse_ts[1:]))

        # ── Save density_pred_wsindy.npz ──
        np.savez_compressed(
            test_run_dir / "density_pred_wsindy.npz",
            rho=U_pred,
            xgrid=test_data["xgrid"],
            ygrid=test_data["ygrid"],
            times=np.asarray(times_fc, dtype=np.float32),
            forecast_start_idx=int(forecast_start * subsample),
        )

        # ── Save r2_vs_time_wsindy.csv ──
        n_save = min(len(times_fc), len(r2_ts))
        pd.DataFrame({
            "time": times_fc[:n_save],
            "r2_reconstructed": r2_ts[:n_save],
            "r2_latent": r2_ts[:n_save],
            "r2_pod": np.ones(n_save),
        }).to_csv(test_run_dir / "r2_vs_time_wsindy.csv", index=False)

        # ── Save metrics_summary_wsindy.json ──
        with open(test_run_dir / "metrics_summary_wsindy.json", "w") as f:
            json.dump({
                "r2_recon": mean_r2, "rmse_recon": mean_rmse,
                "method": method, "n_forecast_steps": n_forecast,
            }, f, indent=2)

        test_results.append({
            "test_id": test_idx, "run_name": run_name,
            "r2_reconstructed": mean_r2, "r2_latent": mean_r2,
            "r2_pod": 1.0,
            "rmse_recon": mean_rmse,
            "forecast_method": method, "n_forecast_steps": n_forecast,
        })

        all_wsindy_r2.append(r2_ts[1:])

        # Load MVAR r2_vs_time for comparison
        for mvar_r2_path in [test_run_dir / "r2_vs_time_mvar.csv",
                             test_run_dir / "r2_vs_time.csv"]:
            if mvar_r2_path.exists():
                mvar_r2_csv = pd.read_csv(mvar_r2_path)
                all_mvar_r2.append(mvar_r2_csv["r2_reconstructed"].values)
                break

        if common_times is None:
            common_times = times_fc[1:]

        ic_type = test_metadata[test_idx].get("distribution",
                    test_metadata[test_idx].get("ic_type", "?"))
        print(f"  [{test_idx+1}/{n_test}] {run_name} ({ic_type}):  "
              f"R²={mean_r2:.4f}  RMSE={mean_rmse:.4e}  ({method})")

    # ================================================================
    # STEP 6: Aggregate results
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 6: Aggregating results")
    print(f"{thin}")

    results_df = pd.DataFrame(test_results)
    results_df.to_csv(WSINDY_DIR / "test_results.csv", index=False)

    wsindy_mean_r2 = results_df["r2_reconstructed"].mean()
    wsindy_std_r2 = results_df["r2_reconstructed"].std()
    print(f"  WSINDy mean R²: {wsindy_mean_r2:.4f} +/- {wsindy_std_r2:.4f}")

    if mvar_df is not None:
        mvar_mean_r2 = mvar_df["r2_reconstructed"].mean()
        mvar_std_r2 = mvar_df["r2_reconstructed"].std()
        diff = wsindy_mean_r2 - mvar_mean_r2
        print(f"  MVAR   mean R²: {mvar_mean_r2:.4f} +/- {mvar_std_r2:.4f}")
        print(f"  Difference    : {diff:+.4f} ({'WSINDy better' if diff > 0 else 'MVAR better'})")

    # ================================================================
    # STEP 7: Generate plots
    # ================================================================
    print(f"\n{thin}")
    print("  STEP 7: Generating plots")
    print(f"{thin}")

    # 1. Discovered PDE coefficients
    p1 = PLOTS_DIR / "discovered_coefficients.png"
    plot_discovered_pde(model, boot_result, p1)
    print(f"  Saved: {p1.name}")

    # 2. Per-test R² bar chart
    p2 = PLOTS_DIR / "r2_per_test_comparison.png"
    plot_r2_per_test(results_df, mvar_df, p2)
    print(f"  Saved: {p2.name}")

    # 3. Mean R² vs time
    if all_wsindy_r2:
        min_len = min(len(r) for r in all_wsindy_r2)
        wsindy_r2_mat = np.array([r[:min_len] for r in all_wsindy_r2])

        mvar_r2_mat = None
        common_times_plot = common_times[:min_len] if common_times is not None else np.arange(min_len)
        if all_mvar_r2:
            min_len_m = min(len(r) for r in all_mvar_r2)
            min_common = min(min_len, min_len_m)
            wsindy_r2_mat = wsindy_r2_mat[:, :min_common]
            mvar_r2_mat = np.array([r[:min_common] for r in all_mvar_r2])
            common_times_plot = common_times_plot[:min_common]

        p3 = PLOTS_DIR / "mean_r2_vs_time.png"
        plot_mean_r2_vs_time(wsindy_r2_mat, mvar_r2_mat, common_times_plot, p3)
        print(f"  Saved: {p3.name}")

    # 4. Density snapshots for first test run
    if test_results and not np.isnan(test_results[0].get("r2_reconstructed", float("nan"))):
        first_run = test_metadata[0]["run_name"]
        first_true = load_density(TEST_DIR / first_run, "density_true.npz")
        first_true_sub = subsample_density(first_true, subsample)
        first_pred_w = np.load(TEST_DIR / first_run / "density_pred_wsindy.npz")

        rho_true_snap = first_true_sub["rho"][mvar_lag:]
        rho_wsindy_snap = first_pred_w["rho"]
        times_snap = first_true_sub["times"][mvar_lag:]

        # Try loading MVAR pred for 3-way comparison
        rho_mvar_snap = None
        for mvar_fname in ["density_pred_mvar.npz", "density_pred.npz"]:
            mvar_path = TEST_DIR / first_run / mvar_fname
            if mvar_path.exists():
                mvar_d = np.load(mvar_path)
                rho_mvar_snap = mvar_d["rho"][mvar_lag:]
                break

        n_fc = min(rho_true_snap.shape[0], rho_wsindy_snap.shape[0])
        if rho_mvar_snap is not None:
            n_fc = min(n_fc, rho_mvar_snap.shape[0])
        snap_idx = [0, n_fc // 4, n_fc // 2, n_fc - 1]

        p4 = PLOTS_DIR / "density_snapshots.png"
        plot_density_snapshots(
            rho_true_snap[:n_fc], rho_wsindy_snap[:n_fc],
            rho_mvar_snap[:n_fc] if rho_mvar_snap is not None else None,
            snap_idx, times_snap, p4,
        )
        print(f"  Saved: {p4.name}")

    # 5. Summary stats panel
    p5 = PLOTS_DIR / "summary_comparison.png"
    plot_summary_stats(results_df, mvar_df, model, p5)
    print(f"  Saved: {p5.name}")

    # ================================================================
    # Save summary JSON
    # ================================================================
    total_time = time.perf_counter() - t_pipeline_start

    summary = {
        "experiment": args.experiment,
        "n_train_runs": len(train_densities),
        "n_test_runs": n_test,
        "subsample": subsample,
        "library": {
            "max_poly": args.max_poly,
            "operators": args.operators,
            "n_terms": len(library_terms),
        },
        "discovered_pde": {
            "text": to_text(model),
            "latex": to_latex(model),
            "active_terms": model.active_terms,
            "coefficients": {
                n: float(model.w[model.col_names.index(n)])
                for n in model.active_terms
            },
            "n_active": model.n_active,
            "lambda_star": float(model.best_lambda),
            "r2_weak": float(model.diagnostics.get("r2", 0)),
        },
        "model_selection": {
            "n_trials": len(sel_result.trials),
            "best_ell": list(sel_result.best.ell),
        },
        "test_evaluation": {
            "mean_r2": float(wsindy_mean_r2) if not np.isnan(wsindy_mean_r2) else None,
            "std_r2": float(wsindy_std_r2) if not np.isnan(wsindy_std_r2) else None,
        },
        "timing": {
            "model_selection_s": round(ms_time, 1),
            "total_s": round(total_time, 1),
        },
    }

    if boot_result is not None:
        summary["bootstrap"] = {
            "B": boot_result["B"],
            "inclusion_probability": {
                n: float(boot_result["inclusion_probability"][i])
                for i, n in enumerate(col_names)
                if boot_result["inclusion_probability"][i] > 0.01
            },
        }

    if mvar_df is not None:
        summary["comparison_with_mvar"] = {
            "mvar_mean_r2": float(mvar_df["r2_reconstructed"].mean()),
            "wsindy_mean_r2": float(wsindy_mean_r2) if not np.isnan(wsindy_mean_r2) else None,
            "difference": float(wsindy_mean_r2 - mvar_df["r2_reconstructed"].mean()),
        }

    with open(WSINDY_DIR / "summary.json", "w") as f:
        json.dump(summary, f, indent=2)

    # ── Final banner ──
    print(f"\n{sep}")
    print("  PIPELINE COMPLETE")
    print(f"{sep}")
    print(f"  Total time      : {total_time:.1f}s")
    print(f"  Discovered PDE  : {to_text(model)}")
    print(f"  WSINDy mean R²  : {wsindy_mean_r2:.4f} +/- {wsindy_std_r2:.4f}")
    if mvar_df is not None:
        print(f"  MVAR   mean R²  : {mvar_df['r2_reconstructed'].mean():.4f}")
    print(f"  Output          : {WSINDY_DIR}")
    print(f"\n  Artifacts:")
    print(f"    {WSINDY_DIR / 'summary.json'}")
    print(f"    {WSINDY_DIR / 'test_results.csv'}")
    for p in sorted(PLOTS_DIR.glob("*.png")):
        print(f"    plots/{p.name}")
    print(f"\n  Per-test artifacts (in {TEST_DIR.name}/test_XXX/):")
    print(f"    density_pred_wsindy.npz")
    print(f"    r2_vs_time_wsindy.csv")
    print(f"    metrics_summary_wsindy.json")
    print(f"{sep}\n")


if __name__ == "__main__":
    main()
