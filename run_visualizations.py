#!/usr/bin/env python3
"""
Visualization Pipeline (Part 2 of 2)
=====================================

Light Visualization Pipeline:
- Loads all data files generated by run_data_generation.py
- Computes evaluation metrics
- Generates plots, videos, and summary reports

This pipeline is designed to be fast and lightweight, suitable for local
execution after heavy data generation on Oscar cluster.

Input: All .npz files, CSVs from data generation pipeline
Output: Videos, plots, metrics CSVs, summary JSON
"""

import numpy as np
from pathlib import Path
import json
import pandas as pd
import argparse
import time

# Import visualization modules
from visualizations import (
    generate_pod_plots,
    compute_test_metrics,
    generate_best_run_visualizations,
    generate_summary_plots,
    generate_time_resolved_analysis,
    generate_summary_json
)


def main():
    parser = argparse.ArgumentParser(description="Visualization Pipeline (Light Computation)")
    parser.add_argument("--experiment_name", type=str, default="test_sim",
                       help="Name of experiment to visualize (must match data generation experiment_name)")
    
    args = parser.parse_args()
    
    # Setup directories with experiment subfolders
    DATA_DIR = Path("oscar_output") / args.experiment_name  # Input: oscar_output/experiment_name
    OUTPUT_DIR = Path("predictions") / args.experiment_name  # Output: predictions/experiment_name
    
    TRAIN_DIR = DATA_DIR / "train"
    TEST_DIR = DATA_DIR / "test"
    MVAR_DIR = DATA_DIR / "mvar"
    
    BEST_RUNS_DIR = OUTPUT_DIR / "best_runs"
    PLOTS_DIR = OUTPUT_DIR / "plots"
    TIME_ANALYSIS_DIR = OUTPUT_DIR / "time_analysis"
    
    # Create output directories
    for d in [BEST_RUNS_DIR, PLOTS_DIR, TIME_ANALYSIS_DIR]:
        d.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("VISUALIZATION PIPELINE (PART 2/2)")
    print("="*80)
    print(f"\nüìÅ Experiment: {args.experiment_name}")
    print(f"üìÅ Data directory: {DATA_DIR}")
    
    # Validate data directory exists
    if not DATA_DIR.exists():
        print(f"\n‚ùå ERROR: Experiment data not found: {DATA_DIR}")
        print("   Run data generation pipeline first:")
        print(f"   python run_data_generation.py --experiment_name {args.experiment_name}")
        return
    
    if not TRAIN_DIR.exists() or not TEST_DIR.exists() or not MVAR_DIR.exists():
        print(f"\n‚ùå ERROR: Missing train/test/mvar directories in {DATA_DIR}")
        print("   Expected structure: train/, test/, mvar/")
        return
    
    pipeline_start = time.time()
    
    # =============================================================================
    # LOAD DATA
    # =============================================================================
    
    print("\n" + "="*80)
    print("Loading Generated Data")
    print("="*80)
    
    # Load training metadata
    with open(TRAIN_DIR / "metadata.json", "r") as f:
        train_metadata = json.load(f)
    
    # Load test metadata
    with open(TEST_DIR / "metadata.json", "r") as f:
        test_metadata = json.load(f)
    
    # Load POD model
    pod_data_raw = np.load(MVAR_DIR / "pod_basis.npz")
    U = pod_data_raw["U"]
    singular_values = pod_data_raw["singular_values"]
    all_singular_values = pod_data_raw["all_singular_values"]
    actual_energy = pod_data_raw["energy_ratio"]
    cumulative_ratio = pod_data_raw["cumulative_ratio"]
    R_POD = U.shape[1]
    
    # Load MVAR model
    mvar_data_raw = np.load(MVAR_DIR / "mvar_model.npz")
    P_LAG = int(mvar_data_raw["p"])
    train_r2 = float(mvar_data_raw["train_r2"])
    train_rmse = float(mvar_data_raw["train_rmse"])
    
    # Load X_train_mean for R¬≤ calculation
    X_train_mean = np.load(MVAR_DIR / "X_train_mean.npy")
    
    # Load simulation config from first training run
    first_run_data = np.load(TRAIN_DIR / train_metadata[0]["run_name"] / "density.npz")
    BASE_CONFIG_SIM = {
        "Lx": float(first_run_data["xgrid"][-1] - first_run_data["xgrid"][0] + 
                    (first_run_data["xgrid"][1] - first_run_data["xgrid"][0])),
        "Ly": float(first_run_data["ygrid"][-1] - first_run_data["ygrid"][0] + 
                    (first_run_data["ygrid"][1] - first_run_data["ygrid"][0]))
    }
    
    N_TRAIN = len(train_metadata)
    M_TEST = len(test_metadata)
    
    # Dynamically detect IC types from test metadata (handle both "ic_type" and "distribution")
    ic_key = "distribution" if "distribution" in test_metadata[0] else "ic_type"
    IC_TYPES = sorted(set(meta[ic_key] for meta in test_metadata))
    
    # For DataFrame access, always use 'ic_type' since compute_test_metrics standardizes it
    df_ic_key = 'ic_type'
    
    # Package POD data with additional parameters
    pod_data = {
        'singular_values': singular_values,
        'all_singular_values': all_singular_values,
        'cumulative_energy': cumulative_ratio,
        'p_lag': P_LAG,
        'train_r2': train_r2,
        'train_rmse': train_rmse
    }
    
    print(f"\n‚úì Loaded data:")
    print(f"   Training: {N_TRAIN} runs")
    print(f"   Test: {M_TEST} runs")
    print(f"   POD: {R_POD} modes ({actual_energy*100:.2f}% energy)")
    print(f"   MVAR: p={P_LAG}, R¬≤={train_r2:.4f}")
    
    # =============================================================================
    # STEP 1: POD Plots
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 1: Generating POD Plots")
    print("="*80)
    
    generate_pod_plots(pod_data, PLOTS_DIR, N_TRAIN)
    
    # =============================================================================
    # STEP 2: Compute Metrics
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 2: Computing Test Metrics")
    print("="*80)
    
    metrics_df, test_predictions, ic_metrics = compute_test_metrics(
        test_metadata=test_metadata,
        test_dir=TEST_DIR,
        x_train_mean=X_train_mean,
        ic_types=IC_TYPES,
        output_dir=OUTPUT_DIR
    )
    
    # =============================================================================
    # STEP 3: Best Run Visualizations
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 3: Generating Best Run Visualizations")
    print("="*80)
    
    top_4_runs = generate_best_run_visualizations(
        metrics_df=metrics_df,
        test_predictions=test_predictions,
        ic_metrics=ic_metrics,
        test_dir=TEST_DIR,
        best_runs_dir=BEST_RUNS_DIR,
        base_config_sim=BASE_CONFIG_SIM,
        p_lag=P_LAG,
        n_top=4
    )
    
    # =============================================================================
    # STEP 4: Summary Plots
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 4: Generating Summary Plots")
    print("="*80)
    
    generate_summary_plots(metrics_df, IC_TYPES, PLOTS_DIR)
    
    # =============================================================================
    # STEP 5: Time-Resolved Analysis
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 5: Time-Resolved Analysis")
    print("="*80)
    
    degradation_info = generate_time_resolved_analysis(
        test_metadata=test_metadata,
        test_dir=TEST_DIR,
        mvar_dir=MVAR_DIR,
        data_dir=DATA_DIR,
        time_analysis_dir=TIME_ANALYSIS_DIR
    )
    
    # =============================================================================
    # STEP 6: Summary JSON
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 6: Generating Summary JSON")
    print("="*80)
    
    summary = generate_summary_json(
        metrics_df=metrics_df,
        ic_metrics=ic_metrics,
        ic_types=IC_TYPES,
        pod_data=pod_data,
        train_metadata=train_metadata,
        test_metadata=test_metadata,
        base_config_sim=BASE_CONFIG_SIM,
        degradation_info=degradation_info,
        output_dir=OUTPUT_DIR
    )
    
    # =============================================================================
    # COMPLETE
    # =============================================================================
    
    total_time = time.time() - pipeline_start
    
    print("\n" + "="*80)
    print("VISUALIZATION COMPLETE! üéâ")
    print("="*80)
    
    print(f"\nüìÇ Input data from: {DATA_DIR}")
    print(f"üìÇ Predictions saved to: {OUTPUT_DIR}")
    print(f"\nüìä Results:")
    print(f"   ‚Ä¢ Mean R¬≤: {metrics_df['r2'].mean():.4f} ¬± {metrics_df['r2'].std():.4f}")
    best_run_name = metrics_df.loc[metrics_df["r2"].idxmax(), "run_name"]
    best_run_r2 = metrics_df["r2"].max()
    print(f"   ‚Ä¢ Best run: {best_run_name} (R¬≤={best_run_r2:.4f})")
    
    print(f"\nüé¨ Top 4 Runs by R¬≤ ({BEST_RUNS_DIR.name}/):")
    for idx, row in top_4_runs.iterrows():
        print(f"   ‚Ä¢ {row[df_ic_key]}/ - {row['run_name']} (R¬≤={row['r2']:.4f})")
    
    print(f"\nüìä Summary Plots ({PLOTS_DIR.name}/):")
    print(f"   ‚Ä¢ POD singular values + energy spectrum")
    print(f"   ‚Ä¢ R¬≤ and error by IC type")
    
    if degradation_info:
        print(f"\n‚è±Ô∏è  Time-Resolved Analysis ({TIME_ANALYSIS_DIR.name}/):")
        print(f"   ‚Ä¢ R¬≤ evolution over time (mean, detailed, survival curves)")
        if 'best_r2' in degradation_info:
            print(f"   ‚Ä¢ Best performance: t={degradation_info['best_time']:.1f}s (R¬≤={degradation_info['best_r2']:.3f})")
        if 'mean_r2_below_0.5' in degradation_info:
            print(f"   ‚Ä¢ R¬≤ drops below 0.5 at t={degradation_info['mean_r2_below_0.5']:.1f}s")
    
    print(f"\n‚è±Ô∏è  Visualization time: {total_time//60:.0f}m {total_time%60:.1f}s")
    print("\n‚úÖ Pipeline complete!")
    print("="*80)


if __name__ == "__main__":
    main()

