#!/usr/bin/env python3
"""
Visualization Pipeline (Part 2 of 2)
=====================================

Light Visualization Pipeline:
- Loads all data files generated by run_data_generation.py
- Computes evaluation metrics
- Generates plots, videos, and summary reports

This pipeline is designed to be fast and lightweight, suitable for local
execution after heavy data generation on Oscar cluster.

Input: All .npz files, CSVs from data generation pipeline
Output: Videos, plots, metrics CSVs, summary JSON
"""

import numpy as np
from pathlib import Path
import json
import pandas as pd
import argparse
import time
import sys
import shutil

# Add src to path for rectsim modules
sys.path.insert(0, str(Path(__file__).parent / 'src'))

# Import visualization modules
from visualizations import (
    generate_pod_plots,
    compute_test_metrics,
    generate_best_run_visualizations,
    generate_summary_plots,
    generate_time_resolved_analysis,
    generate_summary_json
)
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec


def generate_lstm_training_plot(training_log, plots_dir):
    """
    Generate LSTM training convergence plot.
    
    Args:
        training_log: DataFrame with columns ['epoch', 'train_loss', 'val_loss']
        plots_dir: Path to save plots
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    epochs = training_log['epoch'].values
    train_loss = training_log['train_loss'].values
    val_loss = training_log['val_loss'].values
    
    # Loss curves
    ax1.plot(epochs, train_loss, label='Train Loss', linewidth=2, alpha=0.8)
    ax1.plot(epochs, val_loss, label='Val Loss', linewidth=2, alpha=0.8)
    
    # Mark best epoch
    best_epoch = training_log['val_loss'].idxmin()
    best_val_loss = val_loss[best_epoch]
    ax1.scatter([epochs[best_epoch]], [best_val_loss], 
                color='red', s=100, zorder=5, label=f'Best (epoch {epochs[best_epoch]})')
    
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('LSTM Training Convergence', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # Loss improvement rate
    loss_improvement = np.diff(val_loss)
    ax2.plot(epochs[1:], -loss_improvement, linewidth=2, alpha=0.8, color='green')
    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Loss Improvement (-Œî Val Loss)', fontsize=12)
    ax2.set_title('Validation Loss Improvement Rate', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = plots_dir / "lstm_training.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   ‚úì Saved: {output_path.name}")


def generate_mvar_lstm_comparison(mvar_metrics, lstm_metrics, mvar_ic_metrics, lstm_ic_metrics, ic_types, plots_dir):
    """
    Generate 4-panel comparison plot between MVAR and LSTM.
    
    Args:
        mvar_metrics: DataFrame with MVAR test results
        lstm_metrics: DataFrame with LSTM test results
        mvar_ic_metrics: Dict of MVAR metrics by IC type
        lstm_ic_metrics: Dict of LSTM metrics by IC type
        ic_types: List of IC types
        plots_dir: Path to save plots
    """
    fig = plt.figure(figsize=(14, 10))
    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.3)
    
    # Panel 1: R¬≤ by IC type
    ax1 = fig.add_subplot(gs[0, 0])
    x_pos = np.arange(len(ic_types))
    width = 0.35
    
    mvar_r2 = [mvar_ic_metrics[ic]['mean_r2'] for ic in ic_types]
    lstm_r2 = [lstm_ic_metrics[ic]['mean_r2'] for ic in ic_types]
    
    ax1.bar(x_pos - width/2, mvar_r2, width, label='MVAR', alpha=0.8)
    ax1.bar(x_pos + width/2, lstm_r2, width, label='LSTM', alpha=0.8)
    ax1.set_xlabel('IC Type', fontsize=12)
    ax1.set_ylabel('Mean R¬≤', fontsize=12)
    ax1.set_title('R¬≤ by Initial Condition', fontsize=13, fontweight='bold')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(ic_types, rotation=45, ha='right')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    
    # Panel 2: RMSE by IC type
    ax2 = fig.add_subplot(gs[0, 1])
    
    mvar_rmse = [mvar_ic_metrics[ic]['mean_rmse'] for ic in ic_types]
    lstm_rmse = [lstm_ic_metrics[ic]['mean_rmse'] for ic in ic_types]
    
    ax2.bar(x_pos - width/2, mvar_rmse, width, label='MVAR', alpha=0.8)
    ax2.bar(x_pos + width/2, lstm_rmse, width, label='LSTM', alpha=0.8)
    ax2.set_xlabel('IC Type', fontsize=12)
    ax2.set_ylabel('Mean RMSE', fontsize=12)
    ax2.set_title('RMSE by Initial Condition', fontsize=13, fontweight='bold')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(ic_types, rotation=45, ha='right')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # Panel 3: Overall distribution (violin plot)
    ax3 = fig.add_subplot(gs[1, 0])
    
    data_to_plot = [mvar_metrics['r2'].values, lstm_metrics['r2'].values]
    positions = [1, 2]
    
    parts = ax3.violinplot(data_to_plot, positions=positions, showmeans=True, showmedians=True)
    ax3.set_xticks(positions)
    ax3.set_xticklabels(['MVAR', 'LSTM'])
    ax3.set_ylabel('R¬≤', fontsize=12)
    ax3.set_title('R¬≤ Distribution (All Tests)', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    # Panel 4: Summary statistics
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.axis('off')
    
    # Calculate statistics
    mvar_mean = mvar_metrics['r2'].mean()
    mvar_std = mvar_metrics['r2'].std()
    mvar_rmse_mean = mvar_metrics['rmse'].mean()
    
    lstm_mean = lstm_metrics['r2'].mean()
    lstm_std = lstm_metrics['r2'].std()
    lstm_rmse_mean = lstm_metrics['rmse'].mean()
    
    diff = mvar_mean - lstm_mean
    percent_diff = (diff / abs(lstm_mean)) * 100 if lstm_mean != 0 else 0
    
    summary_text = f"""
    SUMMARY STATISTICS
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    MVAR Performance:
      ‚Ä¢ Mean R¬≤:    {mvar_mean:7.4f} ¬± {mvar_std:.4f}
      ‚Ä¢ Mean RMSE:  {mvar_rmse_mean:7.4f}
      ‚Ä¢ N tests:    {len(mvar_metrics)}
    
    LSTM Performance:
      ‚Ä¢ Mean R¬≤:    {lstm_mean:7.4f} ¬± {lstm_std:.4f}
      ‚Ä¢ Mean RMSE:  {lstm_rmse_mean:7.4f}
      ‚Ä¢ N tests:    {len(lstm_metrics)}
    
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    Comparison:
      ‚Ä¢ R¬≤ Difference:  {diff:+.4f}
      ‚Ä¢ Relative:       {percent_diff:+.1f}%
      ‚Ä¢ Winner:         {'MVAR' if diff > 0 else 'LSTM' if diff < 0 else 'TIE'}
    """
    
    ax4.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',
             verticalalignment='center', transform=ax4.transAxes)
    
    plt.suptitle('MVAR vs LSTM: Comprehensive Comparison', fontsize=16, fontweight='bold', y=0.98)
    
    output_path = plots_dir / "mvar_lstm_comparison.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   ‚úì Saved: {output_path.name}")


def generate_latent_vs_lifted_analysis(data_dir, test_dir, test_metadata, models_data, plots_dir):
    """
    Generate latent vs lifted R¬≤ comparison plots.
    
    Uses the OSCAR-generated test_results.csv (which has r2_reconstructed, r2_latent, r2_pod)
    and r2_vs_time.csv files to produce:
    1. Bar chart: latent vs lifted vs POD R¬≤ per model
    2. Time-resolved: latent vs lifted R¬≤ on same axes with error envelope
    3. Spectral radius annotation if available
    """
    plots_dir = Path(plots_dir)
    test_dir = Path(test_dir)
    data_dir = Path(data_dir)
    
    for model_name, model_info in models_data.items():
        model_dir = model_info['dir']
        test_results_csv = model_dir / "test_results.csv"
        
        if not test_results_csv.exists():
            print(f"   ‚ö† No test_results.csv for {model_name.upper()} ‚Äî skipping latent analysis")
            continue
        
        df = pd.read_csv(test_results_csv)
        
        if 'r2_latent' not in df.columns:
            print(f"   ‚ö† No r2_latent column in {model_name.upper()} test_results ‚Äî skipping")
            continue
        
        r2_lifted = df['r2_reconstructed'].mean()
        r2_latent = df['r2_latent'].mean()
        r2_pod = df['r2_pod'].mean()
        gap = r2_latent - r2_lifted
        
        print(f"\n   {model_name.upper()} Latent vs Lifted R¬≤:")
        print(f"      R¬≤ Lifted (density):  {r2_lifted:+.4f}")
        print(f"      R¬≤ Latent (ROM):      {r2_latent:+.4f}")
        print(f"      R¬≤ POD (basis ceil):  {r2_pod:+.4f}")
        print(f"      Lifting gap:          {gap:+.4f} ({'latent > lifted' if gap > 0 else 'lifted > latent'})")
        
        # Spectral radius
        rho = None
        mvar_npz = model_dir / "mvar_model.npz"
        if mvar_npz.exists():
            mvar_data = np.load(mvar_npz)
            if 'A_companion' in mvar_data:
                A_coef = mvar_data['A_companion']
                R_POD = int(mvar_data['r'])
                P_LAG = int(mvar_data['p'])
                companion_dim = P_LAG * R_POD
                C = np.zeros((companion_dim, companion_dim))
                C[:R_POD, :] = A_coef
                for k in range(P_LAG - 1):
                    C[(k+1)*R_POD:(k+2)*R_POD, k*R_POD:(k+1)*R_POD] = np.eye(R_POD)
                rho = np.max(np.abs(np.linalg.eigvals(C)))
                print(f"      Spectral radius:      {rho:.4f}")
        
        # --- Plot 1: Time-resolved latent vs lifted ---
        # Load r2_vs_time.csv for all test runs
        ic_key = 'ic_type' if 'ic_type' in test_metadata[0] else 'distribution'
        all_time_data = []
        for meta in test_metadata:
            run_name = meta['run_name']
            # Try model-specific file first
            r2_file = test_dir / run_name / f"r2_vs_time_{model_name}.csv"
            if not r2_file.exists():
                r2_file = test_dir / run_name / "r2_vs_time.csv"
            if r2_file.exists():
                tdf = pd.read_csv(r2_file)
                tdf['test_id'] = run_name
                tdf['ic_type'] = meta[ic_key]
                all_time_data.append(tdf)
        
        if all_time_data:
            time_df = pd.concat(all_time_data, ignore_index=True)
            times = sorted(time_df['time'].unique())
            
            # Compute statistics
            means_lifted, means_latent, means_pod = [], [], []
            iqr_lifted_lo, iqr_lifted_hi = [], []
            iqr_latent_lo, iqr_latent_hi = [], []
            
            for t in times:
                subset = time_df[time_df['time'] == t]
                means_lifted.append(subset['r2_reconstructed'].mean())
                means_latent.append(subset['r2_latent'].mean())
                means_pod.append(subset['r2_pod'].mean())
                iqr_lifted_lo.append(subset['r2_reconstructed'].quantile(0.25))
                iqr_lifted_hi.append(subset['r2_reconstructed'].quantile(0.75))
                iqr_latent_lo.append(subset['r2_latent'].quantile(0.25))
                iqr_latent_hi.append(subset['r2_latent'].quantile(0.75))
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), height_ratios=[3, 1])
            
            # Top panel: R¬≤ over time for all three metrics
            ax1.plot(times, means_lifted, color='#2E86AB', linewidth=3, 
                    label=f'R¬≤ Lifted (mean={r2_lifted:+.4f})', alpha=0.9, zorder=3)
            ax1.fill_between(times, iqr_lifted_lo, iqr_lifted_hi,
                            color='#2E86AB', alpha=0.15, zorder=1)
            
            ax1.plot(times, means_latent, color='#A23B72', linewidth=3, 
                    label=f'R¬≤ Latent (mean={r2_latent:+.4f})', alpha=0.9, zorder=3)
            ax1.fill_between(times, iqr_latent_lo, iqr_latent_hi,
                            color='#A23B72', alpha=0.15, zorder=1)
            
            ax1.plot(times, means_pod, color='#F18F01', linewidth=2.5, 
                    label=f'R¬≤ POD Ceiling ({r2_pod:.4f})', linestyle='--', alpha=0.7, zorder=2)
            
            ax1.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.4)
            ax1.axhline(0.5, color='orange', linestyle=':', linewidth=1.5, alpha=0.4)
            
            rho_str = f' | spectral radius = {rho:.4f}' if rho else ''
            ax1.set_title(f'{model_name.upper()}: Latent vs Lifted R¬≤ Over Time{rho_str}',
                         fontsize=16, fontweight='bold')
            ax1.set_ylabel('R¬≤ Score', fontsize=14, fontweight='bold')
            ax1.legend(fontsize=12, loc='best', framealpha=0.95)
            ax1.grid(True, alpha=0.3)
            ax1.set_xlim([min(times), max(times)])
            
            # Bottom panel: Gap (latent - lifted) over time
            gaps = [lat - lift for lat, lift in zip(means_latent, means_lifted)]
            ax2.fill_between(times, 0, gaps, where=[g >= 0 for g in gaps],
                            color='#A23B72', alpha=0.4, label='Latent > Lifted')
            ax2.fill_between(times, 0, gaps, where=[g < 0 for g in gaps],
                            color='#2E86AB', alpha=0.4, label='Lifted > Latent')
            ax2.plot(times, gaps, color='black', linewidth=2, alpha=0.8)
            ax2.axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)
            
            ax2.set_xlabel('Time (s)', fontsize=14, fontweight='bold')
            ax2.set_ylabel('Gap (Latent ‚àí Lifted)', fontsize=14, fontweight='bold')
            ax2.set_title('Lifting Gap Over Time', fontsize=13, fontweight='bold')
            ax2.legend(fontsize=10, loc='best')
            ax2.grid(True, alpha=0.3)
            ax2.set_xlim([min(times), max(times)])
            
            plt.tight_layout()
            output_path = plots_dir / f"latent_vs_lifted_r2_{model_name}.png"
            plt.savefig(output_path, dpi=250, bbox_inches='tight')
            plt.close()
            print(f"   ‚úì Saved: {output_path.name}")
        else:
            print(f"   ‚ö† No r2_vs_time.csv files found for {model_name.upper()}")


def export_oscar_insight_data(data_dir, output_dir, models_data):
    """
    Copy all OSCAR-generated insight files to the predictions output folder.
    
    This ensures that all the important metadata, metrics, runtime info,
    and model summaries are available alongside the visualization outputs
    in a single predictions/<experiment>/ folder.
    
    Copied structure:
        predictions/<exp>/oscar_data/
            config_used.yaml          - Exact config that ran on OSCAR
            summary.json              - OSCAR pipeline summary (timing, counts, etc.)
            runtime_comparison.json   - MVAR vs LSTM timing comparison
            MVAR/
                test_results.csv      - Per-test R¬≤ (lifted, latent, POD)
                runtime_profile.json  - MVAR training/eval timing breakdown
                mvar_model.npz        - Model coefficients + spectral info
            LSTM/
                test_results.csv      - Per-test R¬≤ (lifted, latent, POD)
                runtime_profile.json  - LSTM training/eval timing breakdown
                training_log.csv      - Epoch-by-epoch loss convergence
            test/
                metadata.json         - Test run metadata (IC types, params)
                index_mapping.csv     - Maps test indices to run names
                test_XXX/
                    metrics_summary.json - Per-test detailed metrics
                    r2_vs_time.csv       - Time-resolved R¬≤ for this test
    """
    data_dir = Path(data_dir)
    output_dir = Path(output_dir)
    oscar_data_dir = output_dir / "oscar_data"
    oscar_data_dir.mkdir(parents=True, exist_ok=True)
    
    copied = 0
    skipped = 0
    
    # --- Root-level insight files ---
    root_files = [
        "config_used.yaml",
        "summary.json",
        "runtime_comparison.json",
    ]
    for fname in root_files:
        src = data_dir / fname
        dst = oscar_data_dir / fname
        if src.exists():
            shutil.copy2(src, dst)
            copied += 1
        else:
            skipped += 1
    
    # --- Model-level insight files ---
    for model_name in ['MVAR', 'LSTM']:
        model_src = data_dir / model_name
        model_dst = oscar_data_dir / model_name
        
        if not model_src.exists():
            continue
        
        model_dst.mkdir(parents=True, exist_ok=True)
        
        model_files = {
            'MVAR': ['test_results.csv', 'runtime_profile.json', 'mvar_model.npz'],
            'LSTM': ['test_results.csv', 'runtime_profile.json', 'training_log.csv'],
        }
        
        for fname in model_files.get(model_name, []):
            src = model_src / fname
            dst = model_dst / fname
            if src.exists():
                shutil.copy2(src, dst)
                copied += 1
            else:
                skipped += 1
    
    # --- Test-level insight files ---
    test_src = data_dir / "test"
    test_dst = oscar_data_dir / "test"
    
    if test_src.exists():
        test_dst.mkdir(parents=True, exist_ok=True)
        
        # Top-level test files
        for fname in ['metadata.json', 'index_mapping.csv', 'test_results.csv']:
            src = test_src / fname
            dst = test_dst / fname
            if src.exists():
                shutil.copy2(src, dst)
                copied += 1
        
        # Per-test insight files (metrics + r2_vs_time only, NOT density .npz)
        for test_run_dir in sorted(test_src.glob("test_*")):
            if test_run_dir.is_dir():
                dst_run_dir = test_dst / test_run_dir.name
                dst_run_dir.mkdir(parents=True, exist_ok=True)
                
                for fname in ['metrics_summary.json', 'r2_vs_time.csv']:
                    src = test_run_dir / fname
                    dst = dst_run_dir / fname
                    if src.exists():
                        shutil.copy2(src, dst)
                        copied += 1
    
    # --- Generate a combined experiment card ---
    _generate_experiment_card(data_dir, models_data, oscar_data_dir)
    
    print(f"   ‚úì Exported {copied} insight files to {oscar_data_dir.relative_to(output_dir.parent.parent) if output_dir.parent.parent.exists() else oscar_data_dir}")
    if skipped > 0:
        print(f"   ‚ö† {skipped} expected files not found (may be normal for partial runs)")


def _generate_experiment_card(data_dir, models_data, oscar_data_dir):
    """Generate a consolidated experiment_card.json with key info from all sources."""
    data_dir = Path(data_dir)
    oscar_data_dir = Path(oscar_data_dir)
    
    card = {
        'experiment_name': data_dir.name,
        'generated_by': 'run_visualizations.py (Step 10)',
    }
    
    # Pull from summary.json
    summary_path = data_dir / 'summary.json'
    if summary_path.exists():
        with open(summary_path) as f:
            summary = json.load(f)
        card['n_train'] = summary.get('n_train')
        card['n_test'] = summary.get('n_test')
        card['R_POD'] = summary.get('R_POD')
        card['models_trained'] = summary.get('models_trained', [])
        card['total_time_minutes'] = summary.get('total_time_minutes')
        card['timestamp'] = summary.get('timestamp')
    
    # Pull from runtime_comparison.json
    runtime_path = data_dir / 'runtime_comparison.json'
    if runtime_path.exists():
        with open(runtime_path) as f:
            card['runtime_comparison'] = json.load(f)
    
    # Pull per-model highlights
    card['models'] = {}
    for model_name, model_info in models_data.items():
        model_dir = model_info['dir']
        model_card = {}
        
        # Test results summary
        test_csv = model_dir / 'test_results.csv'
        if test_csv.exists():
            df = pd.read_csv(test_csv)
            model_card['n_tests'] = len(df)
            for col in ['r2_reconstructed', 'r2_latent', 'r2_pod']:
                if col in df.columns:
                    model_card[f'{col}_mean'] = round(float(df[col].mean()), 6)
                    model_card[f'{col}_std'] = round(float(df[col].std()), 6)
                    model_card[f'{col}_median'] = round(float(df[col].median()), 6)
        
        # Runtime profile
        runtime_file = model_dir / 'runtime_profile.json'
        if runtime_file.exists():
            with open(runtime_file) as f:
                model_card['runtime_profile'] = json.load(f)
        
        # MVAR spectral radius
        if model_name == 'mvar':
            mvar_npz = model_dir / 'mvar_model.npz'
            if mvar_npz.exists():
                mvar_data = np.load(mvar_npz)
                if 'A_companion' in mvar_data:
                    A_coef = mvar_data['A_companion']
                    R = int(mvar_data['r'])
                    P = int(mvar_data['p'])
                    dim = P * R
                    C = np.zeros((dim, dim))
                    C[:R, :] = A_coef
                    for k in range(P - 1):
                        C[(k+1)*R:(k+2)*R, k*R:(k+1)*R] = np.eye(R)
                    eigs = np.abs(np.linalg.eigvals(C))
                    model_card['spectral_radius'] = round(float(np.max(eigs)), 6)
                    model_card['n_unstable_eigenvalues'] = int(np.sum(eigs > 1.0))
                    model_card['n_total_eigenvalues'] = len(eigs)
                model_card['p_lag'] = int(mvar_data['p'])
                model_card['r_pod'] = int(mvar_data['r'])
                model_card['alpha'] = float(mvar_data['alpha'])
                model_card['train_r2'] = round(float(mvar_data['train_r2']), 6)
        
        card['models'][model_name.upper()] = model_card
    
    # Save
    card_path = oscar_data_dir / 'experiment_card.json'
    with open(card_path, 'w') as f:
        json.dump(card, f, indent=2)


def main():
    parser = argparse.ArgumentParser(description="Visualization Pipeline (Light Computation)")
    parser.add_argument("--experiment_name", type=str, default="test_sim",
                       help="Name of experiment to visualize (must match data generation experiment_name)")
    
    args = parser.parse_args()
    
    # Setup directories with experiment subfolders
    DATA_DIR = Path("oscar_output") / args.experiment_name  # Input: oscar_output/experiment_name
    OUTPUT_DIR = Path("predictions") / args.experiment_name  # Output: predictions/experiment_name
    
    TRAIN_DIR = DATA_DIR / "train"
    TEST_DIR = DATA_DIR / "test"
    
    # Detect structure: old (mvar/) or unified (rom_common/, MVAR/, LSTM/)
    OLD_MVAR_DIR = DATA_DIR / "mvar"
    ROM_COMMON_DIR = DATA_DIR / "rom_common"
    MVAR_DIR = DATA_DIR / "MVAR"
    LSTM_DIR = DATA_DIR / "LSTM"
    
    # Determine which structure we're using
    use_unified_structure = ROM_COMMON_DIR.exists()
    
    if use_unified_structure:
        print(f"\nüîç Detected unified multi-model structure")
        POD_DIR = ROM_COMMON_DIR
        MODEL_DIRS = {}
        if (MVAR_DIR / "mvar_model.npz").exists():
            MODEL_DIRS['mvar'] = MVAR_DIR
        elif MVAR_DIR.exists():
            print(f"  ‚ö†Ô∏è  MVAR/ directory exists but mvar_model.npz not found ‚Äî skipping MVAR")
        if LSTM_DIR.exists() and any(LSTM_DIR.iterdir()):
            MODEL_DIRS['lstm'] = LSTM_DIR
        elif LSTM_DIR.exists():
            print(f"  ‚ö†Ô∏è  LSTM/ directory exists but is empty ‚Äî skipping LSTM")
    else:
        print(f"\nüîç Detected legacy single-model structure")
        POD_DIR = OLD_MVAR_DIR
        MODEL_DIRS = {'mvar': OLD_MVAR_DIR}
    
    BEST_RUNS_DIR = OUTPUT_DIR / "best_runs"
    PLOTS_DIR = OUTPUT_DIR / "plots"
    TIME_ANALYSIS_DIR = OUTPUT_DIR / "time_analysis"
    
    # Create output directories
    for d in [BEST_RUNS_DIR, PLOTS_DIR, TIME_ANALYSIS_DIR]:
        d.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("VISUALIZATION PIPELINE (PART 2/2)")
    print("="*80)
    print(f"\nüìÅ Experiment: {args.experiment_name}")
    print(f"üìÅ Data directory: {DATA_DIR}")
    
    # Validate data directory exists
    if not DATA_DIR.exists():
        print(f"\n‚ùå ERROR: Experiment data not found: {DATA_DIR}")
        print("   Run data generation pipeline first:")
        print(f"   python run_data_generation.py --experiment_name {args.experiment_name}")
        return
    
    if not TRAIN_DIR.exists() or not TEST_DIR.exists():
        print(f"\n‚ùå ERROR: Missing train/test directories in {DATA_DIR}")
        return
    
    if not OLD_MVAR_DIR.exists() and not ROM_COMMON_DIR.exists():
        print(f"\n‚ùå ERROR: No model directories found in {DATA_DIR}")
        print("   Expected either: mvar/ (old) or rom_common/ (unified)")
        return
    
    pipeline_start = time.time()
    
    # =============================================================================
    # LOAD DATA
    # =============================================================================
    
    print("\n" + "="*80)
    print("Loading Generated Data")
    print("="*80)
    
    # Load training metadata
    with open(TRAIN_DIR / "metadata.json", "r") as f:
        train_metadata = json.load(f)
    
    # Load test metadata
    with open(TEST_DIR / "metadata.json", "r") as f:
        test_metadata = json.load(f)
    
    # Load POD model from appropriate directory
    pod_data_raw = np.load(POD_DIR / "pod_basis.npz")
    U = pod_data_raw["U"]
    singular_values = pod_data_raw["singular_values"]
    all_singular_values = pod_data_raw["all_singular_values"]
    actual_energy = pod_data_raw["energy_ratio"]
    cumulative_ratio = pod_data_raw["cumulative_ratio"]
    R_POD = U.shape[1]
    
    # Load X_train_mean from appropriate directory
    if use_unified_structure:
        X_train_mean = np.load(ROM_COMMON_DIR / "X_train_mean.npy")
    else:
        X_train_mean = np.load(OLD_MVAR_DIR / "X_train_mean.npy")
    
    # Load model-specific data
    models_data = {}
    
    # Load MVAR data if available
    if 'mvar' in MODEL_DIRS:
        mvar_dir = MODEL_DIRS['mvar']
        mvar_data_raw = np.load(mvar_dir / "mvar_model.npz")
        models_data['mvar'] = {
            'p_lag': int(mvar_data_raw["p"]),
            'train_r2': float(mvar_data_raw["train_r2"]),
            'train_rmse': float(mvar_data_raw["train_rmse"]),
            'dir': mvar_dir
        }
        P_LAG = models_data['mvar']['p_lag']
        train_r2 = models_data['mvar']['train_r2']
        train_rmse = models_data['mvar']['train_rmse']
    
    # Load LSTM data if available
    if 'lstm' in MODEL_DIRS:
        lstm_dir = MODEL_DIRS['lstm']
        # Load training log
        if (lstm_dir / "training_log.csv").exists():
            lstm_training_log = pd.read_csv(lstm_dir / "training_log.csv")
        else:
            lstm_training_log = None
        
        models_data['lstm'] = {
            'training_log': lstm_training_log,
            'dir': lstm_dir
        }
    
    # Load simulation config from first training run
    first_run_data = np.load(TRAIN_DIR / train_metadata[0]["run_name"] / "density.npz")
    BASE_CONFIG_SIM = {
        "Lx": float(first_run_data["xgrid"][-1] - first_run_data["xgrid"][0] + 
                    (first_run_data["xgrid"][1] - first_run_data["xgrid"][0])),
        "Ly": float(first_run_data["ygrid"][-1] - first_run_data["ygrid"][0] + 
                    (first_run_data["ygrid"][1] - first_run_data["ygrid"][0]))
    }
    
    N_TRAIN = len(train_metadata)
    M_TEST = len(test_metadata)
    
    # Dynamically detect IC types from test metadata (handle both "ic_type" and "distribution")
    ic_key = "distribution" if "distribution" in test_metadata[0] else "ic_type"
    IC_TYPES = sorted(set(meta[ic_key] for meta in test_metadata))
    
    # For DataFrame access, always use 'ic_type' since compute_test_metrics standardizes it
    df_ic_key = 'ic_type'
    
    # Package POD data with additional parameters
    pod_data = {
        'singular_values': singular_values,
        'all_singular_values': all_singular_values,
        'cumulative_energy': cumulative_ratio,
    }
    
    # Add MVAR params if available (for backward compatibility)
    if 'mvar' in models_data:
        pod_data['p_lag'] = models_data['mvar']['p_lag']
        pod_data['train_r2'] = models_data['mvar']['train_r2']
        pod_data['train_rmse'] = models_data['mvar']['train_rmse']
    
    print(f"\n‚úì Loaded data:")
    print(f"   Training: {N_TRAIN} runs")
    print(f"   Test: {M_TEST} runs")
    print(f"   POD: {R_POD} modes ({actual_energy*100:.2f}% energy)")
    
    if 'mvar' in models_data:
        print(f"   MVAR: p={models_data['mvar']['p_lag']}, R¬≤={models_data['mvar']['train_r2']:.4f}")
    if 'lstm' in models_data:
        if models_data['lstm']['training_log'] is not None:
            final_val_loss = models_data['lstm']['training_log']['val_loss'].iloc[-1]
            print(f"   LSTM: final val_loss={final_val_loss:.4f}")
        else:
            print(f"   LSTM: loaded")
    
    # =============================================================================
    # STEP 1: POD Plots
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 1: Generating POD Plots")
    print("="*80)
    
    generate_pod_plots(pod_data, PLOTS_DIR, N_TRAIN)
    
    # =============================================================================
    # STEP 2: Compute Metrics
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 2: Computing Test Metrics")
    print("="*80)
    
    # Compute metrics for each model
    all_metrics = {}
    all_test_predictions = {}
    all_ic_metrics = {}
    
    for model_name, model_info in models_data.items():
        print(f"\n   Computing metrics for {model_name.upper()}...")
        
        # For unified structure, check if test_results.csv exists
        # For legacy structure, always compute from density files
        test_results_path = model_info['dir'] / "test_results.csv"
        
        # Always compute metrics from density files (works for both old and new structure)
        metrics_df, test_predictions, ic_metrics = compute_test_metrics(
            test_metadata=test_metadata,
            test_dir=TEST_DIR,
            x_train_mean=X_train_mean,
            ic_types=IC_TYPES,
            output_dir=OUTPUT_DIR,
            model_name=model_name
        )
        
        all_metrics[model_name] = metrics_df
        all_test_predictions[model_name] = test_predictions
        all_ic_metrics[model_name] = ic_metrics
    
    # Check if we have any metrics
    if not all_metrics:
        print("\n‚ùå ERROR: No metrics computed for any model")
        return
    
    # Use primary model (mvar if available, else first model) for main visualizations
    primary_model = 'mvar' if 'mvar' in all_metrics else list(all_metrics.keys())[0]
    metrics_df = all_metrics[primary_model]
    test_predictions = all_test_predictions[primary_model]
    ic_metrics = all_ic_metrics[primary_model]
    
    # =============================================================================
    # STEP 3: Best Run Visualizations (per model)
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 3: Generating Best Run Visualizations")
    print("="*80)
    
    all_top_runs = {}
    for model_name in all_metrics.keys():
        print(f"\n   Generating best runs for {model_name.upper()}...")
        
        model_best_runs_dir = BEST_RUNS_DIR / model_name.upper()
        model_best_runs_dir.mkdir(exist_ok=True, parents=True)
        
        # Get p_lag for this model (MVAR only)
        model_p_lag = models_data[model_name].get('p_lag', 5) if model_name == 'mvar' else 5
        
        top_runs = generate_best_run_visualizations(
            metrics_df=all_metrics[model_name],
            test_predictions=all_test_predictions[model_name],
            ic_metrics=all_ic_metrics[model_name],
            test_dir=TEST_DIR,
            best_runs_dir=model_best_runs_dir,
            base_config_sim=BASE_CONFIG_SIM,
            p_lag=model_p_lag,
            n_top=4,
            model_name=model_name
        )
        all_top_runs[model_name] = top_runs
    
    # Use primary model for backward compatibility
    top_4_runs = all_top_runs[primary_model]
    
    # =============================================================================
    # STEP 4: Summary Plots
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 4: Generating Summary Plots")
    print("="*80)
    
    generate_summary_plots(metrics_df, IC_TYPES, PLOTS_DIR)
    
    # =============================================================================
    # STEP 5: Time-Resolved Analysis (per model)
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 5: Time-Resolved Analysis")
    print("="*80)
    
    all_degradation_info = {}
    for model_name in all_metrics.keys():
        print(f"\n   Analyzing time evolution for {model_name.upper()}...")
        
        model_time_dir = TIME_ANALYSIS_DIR / model_name.upper()
        model_time_dir.mkdir(exist_ok=True, parents=True)
        
        model_dir = models_data[model_name]['dir']
        
        degradation_info = generate_time_resolved_analysis(
            test_metadata=test_metadata,
            test_dir=TEST_DIR,
            mvar_dir=model_dir,
            data_dir=DATA_DIR,
            time_analysis_dir=model_time_dir,
            model_name=model_name
        )
        all_degradation_info[model_name] = degradation_info
    
    # Use primary model for backward compatibility
    degradation_info = all_degradation_info.get(primary_model, {})
    
    # =============================================================================
    # STEP 6: Summary JSON
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 6: Generating Summary JSON")
    print("="*80)
    
    summary = generate_summary_json(
        metrics_df=metrics_df,
        ic_metrics=ic_metrics,
        ic_types=IC_TYPES,
        pod_data=pod_data,
        train_metadata=train_metadata,
        test_metadata=test_metadata,
        base_config_sim=BASE_CONFIG_SIM,
        degradation_info=degradation_info,
        output_dir=OUTPUT_DIR,
        all_metrics=all_metrics,
        models_data=models_data
    )
    
    # =============================================================================
    # STEP 7: LSTM Training Plot (if LSTM model exists)
    # =============================================================================
    
    if 'lstm' in models_data and models_data['lstm']['training_log'] is not None:
        print("\n" + "="*80)
        print("STEP 7: Generating LSTM Training Plot")
        print("="*80)
        
        generate_lstm_training_plot(
            training_log=models_data['lstm']['training_log'],
            plots_dir=PLOTS_DIR
        )
    
    # =============================================================================
    # STEP 8: MVAR vs LSTM Comparison (if both models exist)
    # =============================================================================
    
    if 'mvar' in all_metrics and 'lstm' in all_metrics:
        print("\n" + "="*80)
        print("STEP 8: Generating MVAR vs LSTM Comparison")
        print("="*80)
        
        generate_mvar_lstm_comparison(
            mvar_metrics=all_metrics['mvar'],
            lstm_metrics=all_metrics['lstm'],
            mvar_ic_metrics=all_ic_metrics['mvar'],
            lstm_ic_metrics=all_ic_metrics['lstm'],
            ic_types=IC_TYPES,
            plots_dir=PLOTS_DIR
        )
    
    # =============================================================================
    # STEP 9: Latent vs Lifted R¬≤ Analysis
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 9: Latent vs Lifted R¬≤ Analysis")
    print("="*80)
    
    generate_latent_vs_lifted_analysis(
        data_dir=DATA_DIR,
        test_dir=TEST_DIR,
        test_metadata=test_metadata,
        models_data=models_data,
        plots_dir=PLOTS_DIR,
    )
    
    # =============================================================================
    # STEP 10: Export OSCAR Insight Data to Predictions
    # =============================================================================
    
    print("\n" + "="*80)
    print("STEP 10: Exporting OSCAR Insight Data")
    print("="*80)
    
    export_oscar_insight_data(
        data_dir=DATA_DIR,
        output_dir=OUTPUT_DIR,
        models_data=models_data,
    )
    
    # =============================================================================
    # COMPLETE
    # =============================================================================
    
    total_time = time.time() - pipeline_start
    
    print("\n" + "="*80)
    print("VISUALIZATION COMPLETE! üéâ")
    print("="*80)
    
    print(f"\nüìÇ Input data from: {DATA_DIR}")
    print(f"üìÇ Predictions saved to: {OUTPUT_DIR}")
    print(f"\nüìä Results:")
    print(f"   ‚Ä¢ Mean R¬≤: {metrics_df['r2'].mean():.4f} ¬± {metrics_df['r2'].std():.4f}")
    best_run_name = metrics_df.loc[metrics_df["r2"].idxmax(), "run_name"]
    best_run_r2 = metrics_df["r2"].max()
    print(f"   ‚Ä¢ Best run: {best_run_name} (R¬≤={best_run_r2:.4f})")
    
    print(f"\nüé¨ Top 4 Runs by R¬≤:")
    for model_name, top_runs in all_top_runs.items():
        print(f"   {model_name.upper()} ({BEST_RUNS_DIR.name}/{model_name.upper()}/)")
        for idx, row in top_runs.iterrows():
            print(f"      ‚Ä¢ {row[df_ic_key]}/ - {row['run_name']} (R¬≤={row['r2']:.4f})")
    
    print(f"\nüìä Summary Plots ({PLOTS_DIR.name}/):")
    print(f"   ‚Ä¢ POD singular values + energy spectrum")
    print(f"   ‚Ä¢ R¬≤ and error by IC type")
    
    if 'lstm' in models_data:
        print(f"   ‚Ä¢ LSTM training convergence")
    
    if 'mvar' in all_metrics and 'lstm' in all_metrics:
        print(f"   ‚Ä¢ MVAR vs LSTM comparison (4 panels)")
        mvar_mean = all_metrics['mvar']['r2'].mean()
        lstm_mean = all_metrics['lstm']['r2'].mean()
        diff = mvar_mean - lstm_mean
        print(f"     - MVAR: R¬≤={mvar_mean:.4f}")
        print(f"     - LSTM: R¬≤={lstm_mean:.4f}")
        print(f"     - Difference: {diff:+.4f}")
    
    if all_degradation_info:
        print(f"\n‚è±Ô∏è  Time-Resolved Analysis:")
        for model_name, deg_info in all_degradation_info.items():
            if deg_info:
                print(f"   {model_name.upper()} ({TIME_ANALYSIS_DIR.name}/{model_name.upper()}/)")
                print(f"      ‚Ä¢ R¬≤ evolution over time (mean, detailed, survival curves)")
                if 'best_r2' in deg_info:
                    print(f"      ‚Ä¢ Best: t={deg_info['best_time']:.1f}s (R¬≤={deg_info['best_r2']:.3f})")
                if 'mean_r2_below_0.5' in deg_info:
                    print(f"      ‚Ä¢ R¬≤ < 0.5 at t={deg_info['mean_r2_below_0.5']:.1f}s")
    
    print(f"\nüìÅ OSCAR Insight Data:")
    print(f"   ‚Ä¢ Exported to {OUTPUT_DIR.name}/oscar_data/")
    oscar_card = OUTPUT_DIR / "oscar_data" / "experiment_card.json"
    if oscar_card.exists():
        print(f"   ‚Ä¢ experiment_card.json ‚Äî consolidated experiment summary")
    
    print(f"\n‚è±Ô∏è  Visualization time: {total_time//60:.0f}m {total_time%60:.1f}s")
    print("\n‚úÖ Pipeline complete!")
    print("="*80)


if __name__ == "__main__":
    main()

