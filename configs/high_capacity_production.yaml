# ============================================================================
# HIGH CAPACITY ROM CONFIG - PRODUCTION VERSION
# ============================================================================
# Higher resolution, more particles, energy-based POD selection
#
# Key design principles:
#   - Training horizon: T = 10.0s (100 timesteps)
#   - N = 100 particles (2.5x increase)
#   - KDE bandwidth: 4.0 grid cells (stronger smoothing)
#   - Density grid: 32×32 (coarser, faster)
#   - POD: energy_threshold = 0.99 (adaptive mode selection)
#   - MVAR: lag w = 4 (shorter memory)
#   - Ridge: alpha = 1e-4 (strong regularization)
#   - Test horizon: 25s (15s extrapolation beyond 10s conditioning)
#
# Identifiability (estimated for d≈30-40 modes):
#   K = T/dt = 10.0/0.1 = 100 steps per run
#   Windows per run ≈ K - w = 96
#   Total windows: 1008 runs × 96 ≈ 96,768
#   Parameters (d=35): d²×w = 35²×4 = 4,900
#   Ratio ρ ≈ 96,768/4,900 ≈ 19.7 ✓ (well-conditioned)
# ============================================================================

# ---------------------------------------------------------------------------
# MICRO MODEL
# ---------------------------------------------------------------------------
sim:
  N: 100                    # 100 particles (increased from 40)
  Lx: 15.0
  Ly: 15.0
  bc: "periodic"
  T: 10.0                   # Training horizon: 10 seconds
  dt: 0.1
  v0: 1.0
  R: 2.0                    # Vicsek interaction radius
  eta: 0.3                  # Noise level

# ---------------------------------------------------------------------------
# DENSITY COMPUTATION (coarser grid, more smoothing)
# ---------------------------------------------------------------------------
density:
  nx: 32                    # 32×32 grid (coarser, faster)
  ny: 32
  bandwidth: 4.0            # 4 grid cells → strong smoothing

# ---------------------------------------------------------------------------
# TRAINING ICs (rich mix, 1008 runs total)
# ---------------------------------------------------------------------------
train_ic:
  gaussian:
    enabled: true
    n_runs: 100  # ignored, uses combinations below
    positions_x: [3.75, 7.5, 11.25]
    positions_y: [3.75, 7.5, 11.25]
    variances: [0.25, 1.0, 4.0, 9.0]  # std: 0.5, 1.0, 2.0, 3.0
    n_samples_per_config: 3  # 3x3x4x3 = 108 runs
  
  uniform:
    enabled: true
    n_runs: 100
  
  ring:
    enabled: true
    n_runs: 100  # ignored, uses combinations below
    radii: [2.0, 3.0, 4.0, 5.0]
    widths: [0.3, 0.3, 0.6, 0.6]
    n_samples_per_config: 25  # 4x25 = 100 runs (samples from 4 ring configs)
  
  two_clusters:
    enabled: true
    n_runs: 100  # ignored, uses combinations below
    separations: [3.0, 4.5, 6.0, 7.5]
    sigmas: [0.8, 0.8, 1.5, 1.5]
    n_samples_per_config: 25  # 4x25 = 100 runs

# ---------------------------------------------------------------------------
# TEST ICs (out-of-training configurations)
# ---------------------------------------------------------------------------
test_ic:
  gaussian:
    enabled: true
    n_runs: 10
    test_positions_x: [5.5, 9.0, 2.0]
    test_positions_y: [5.5, 9.0, 2.0]
    test_variances: [2.25]  # std: 1.5
  
  uniform:
    enabled: true
    n_runs: 10
  
  ring:
    enabled: true
    n_runs: 10
    test_radii: [2.5, 3.5, 4.5]
    test_widths: [0.45, 0.45, 0.45]
    n_samples_per_config: 1
  
  two_clusters:
    enabled: true
    n_runs: 10
    test_separations: [3.75, 5.25, 6.75]
    test_sigmas: [1.1, 1.1, 1.1]

test_sim:
  T: 25.0                   # Test horizon: 25 seconds (2.5x training, 15s forecast!)

# ---------------------------------------------------------------------------
# ROM: ENERGY-BASED POD SELECTION WITH MODEL CONFIGURATION
# ---------------------------------------------------------------------------
rom:
  energy_threshold: 0.99    # Adaptive: select modes capturing 99% energy
  eigenvalue_threshold: 0.999  # Mild eigenvalue scaling (safety net)
  
  models:
    mvar:
      enabled: true         # Enable MVAR model
      lag: 4                # Lag w = 4 (shorter memory, fewer parameters)
      ridge_alpha: 1.0e-4   # Strong ridge regularization (key for stability)
    
    lstm:
      enabled: false        # Disable LSTM by default
      lag: 4
      hidden_units: 64
      num_layers: 2
      activation: "tanh"
      batch_size: 32
      learning_rate: 0.001
      max_epochs: 100
      patience: 10
      weight_decay: 0.0
      gradient_clip: 1.0
      loss: "mse"

# Legacy MVAR settings (for backward compatibility, will be deprecated)
mvar:
  order: 4                  # Lag w = 4 (shorter memory, fewer parameters)
  ridge_alpha: 1.0e-4       # Strong ridge regularization (key for stability)
  enforce_stability: true
  eigenvalue_threshold: 0.999  # Mild eigenvalue scaling (safety net)
  train_fraction: 0.8       # Use first 80% of each trajectory for training

# ---------------------------------------------------------------------------
# EVALUATION (measure extrapolation from end of training)
# ---------------------------------------------------------------------------
eval:
  forecast_dt: 1
  save_time_resolved: true
  metrics:
    - "mse"
    - "mae"
    - "r2"
    - "mass_conservation"

# ---------------------------------------------------------------------------
# QUALITATIVE EFFECTS
# ---------------------------------------------------------------------------
# - More particles (N=100) → richer dynamics, more complex patterns
# - Coarser grid (32×32) → faster computation, less memory
# - Larger bandwidth (4.0) → stronger smoothing, lower effective rank
# - Energy-based POD (0.99) → adaptive mode count based on data complexity
# - Shorter lag (w=4) → fewer parameters, better identifiability
# - Longer forecast (15s) → tests long-term stability and extrapolation
