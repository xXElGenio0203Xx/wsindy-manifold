---
# ============================================================================
# SYNTHESIS V2.4.2 — V2.4 Champion with Low Regularization (α=0.01)
# ============================================================================
#
# HYPOTHESIS:
#   V2.4 got R²=+0.272 with α=10.0 (strong ridge).
#   Does the MVAR need that much regularization, or is α=10 
#   over-shrinking the coefficients and hiding dynamics?
#
#   α=0.01 is 1000× less regularization — lets MVAR fit freely.
#   With d=10 (only 310 params) and 261 training runs, there's
#   plenty of data to avoid overfitting even with weak ridge.
#
# CHANGES FROM V2.4:
#   - ridge_alpha: 10.0 → 0.01
#   - Everything else identical
#
# ============================================================================

experiment_name: "synthesis_v2_4_2"

# ============================================================================
# SIMULATION — Same physics as V2.4
# ============================================================================
sim:
  N: 100
  T: 6.0
  dt: 0.04
  Lx: 15.0
  Ly: 15.0
  bc: "periodic"

model:
  type: "discrete"
  speed: 1.5
  speed_mode: "constant_with_forces"

params:
  R: 2.5

noise:
  kind: "gaussian"
  eta: 0.2
  match_variance: true

forces:
  enabled: true
  type: "morse"
  params:
    Ca: 0.8
    Cr: 0.3
    la: 1.5
    lr: 0.5
    mu_t: 0.3
    rcut_factor: 5.0

alignment:
  enabled: true

# ============================================================================
# DENSITY
# ============================================================================
density:
  nx: 48
  ny: 48
  bandwidth: 4.0

outputs:
  density_resolution: 48
  density_bandwidth: 4.0

# ============================================================================
# TRAINING ICs — Same 261 runs
# ============================================================================
train_ic:
  type: "mixed_comprehensive"

  gaussian:
    enabled: true
    n_runs: 180
    positions_x: [3.0, 6.0, 9.0, 12.0]
    positions_y: [3.0, 6.0, 9.0, 12.0]
    variances: [0.8, 1.5, 2.5]
    n_samples_per_config: 3

  uniform:
    enabled: true
    n_runs: 75
    n_samples: 75

  two_clusters:
    enabled: true
    n_runs: 45
    separations: [3.0, 5.0, 7.0]
    sigmas: [1.0, 2.0]
    n_samples_per_config: 7

  ring:
    enabled: false

# ============================================================================
# TEST ICs — Same 26 runs
# ============================================================================
test_ic:
  type: "mixed_test_comprehensive"

  gaussian:
    enabled: true
    n_runs: 15
    test_positions_x: [4.5, 7.5, 10.5]
    test_positions_y: [4.5, 7.5, 10.5]
    test_variances: [1.2]
    n_samples_per_config: 1
    extrapolation_positions: [[2.0, 2.0], [13.0, 2.0], [2.0, 13.0], [13.0, 13.0], [7.5, 1.5], [1.5, 7.5]]
    extrapolation_variance: [1.2]

  uniform:
    enabled: true
    n_runs: 8

  two_clusters:
    enabled: true
    n_runs: 7
    test_separations: [4.0, 6.0]
    test_sigmas: [1.5]
    n_samples_per_config: 3
    extrapolation_separations: [2.0]
    extrapolation_sigma: [1.5]

  ring:
    enabled: false

# ============================================================================
# ROM — Same as V2.4 but α=0.01
# ============================================================================
rom:
  subsample: 3
  fixed_modes: 10

  # NO eigenvalue_threshold — proven winner

  models:
    mvar:
      enabled: true
      lag: 3
      ridge_alpha: 0.01     # 1000× LESS regularization than V2.4

    lstm:
      enabled: true
      lag: 3
      hidden_units: 64
      num_layers: 2
      max_epochs: 3000
      patience: 100
      batch_size: 128
      learning_rate: 1.0e-3
      weight_decay: 1.0e-4
      gradient_clip: 1.0
      dropout: 0.1

# ============================================================================
# EVALUATION — WITH clamping (V2.4 strategy)
# ============================================================================
eval:
  metrics: ["r2", "rmse"]
  save_forecasts: true
  save_time_resolved: true
  forecast_start: 0.36
  clamp_negative: true
