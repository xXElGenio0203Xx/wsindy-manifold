---
# ============================================================================
# SYNTHESIS V4 — MEGA HEAVYWEIGHT
# ============================================================================
#
# PHILOSOPHY: Throw everything at the wall. We have 48 hours.
#
#   "I do not care if it takes more hours"
#
# DESIGN:
#   - 1000 Gaussian ICs × T=60s → longest, most diverse training ever
#   - d=35 modes (98% energy) — matching Alvarez energy threshold
#   - p=7 lag (0.84s memory) — captures slow dynamics
#   - α=1e-4 — light ridge, let the data speak
#   - NO eigenvalue scaling — raw MVAR dynamics
#   - NO clamping — raw predictions
#   - LSTM: 256 hidden, 4 layers, 10000 epochs — monster network
#
# MVAR MATH:
#   d=35, p=7: n_params = 35 × 7 × 35 + 35 = 8,610
#   1000 ICs × T=60s, dt=0.04, subsample=3 → 500 frames/IC
#   Usable samples per IC: 500 - 7 = 493
#   Total samples: 1000 × 493 = 493,000
#   Overdetermination ratio: 493,000 / 8,610 = 57.3× ← EXCELLENT
#
# LSTM MATH:
#   Input: 7 × 35 = 245 features per step
#   Hidden: 256 units, 4 layers
#   Total params: ~800K
#   Training samples: 493,000 → ratio = 616× ← no overfitting risk
#
# TIME BUDGET (48 hours):
#   Sims: 1000 ICs × ~50s each ≈ 14 hours (T=60 is 10× base)
#   POD SVD: ~30 min (1000 × 500 × 2304 matrix)
#   MVAR Ridge: ~30 min (493K × 245 matrix, very large)
#   LSTM: ~6-8 hours (10K epochs, 493K samples, 256h×4L)
#   Test eval: 100 ICs × ~50s ≈ 1.5 hours
#   Total estimate: ~24-28 hours — safe within 48 hour limit
#
# ============================================================================

experiment_name: "synthesis_v4_megaheavyweight"

# ============================================================================
# SIMULATION — Long trajectories
# ============================================================================
sim:
  N: 100
  T: 60.0              # 10× longer than V2 — maximum dynamics coverage
  dt: 0.04
  Lx: 15.0
  Ly: 15.0
  bc: "periodic"

model:
  type: "discrete"
  speed: 1.5
  speed_mode: "constant_with_forces"

params:
  R: 2.5

noise:
  kind: "gaussian"
  eta: 0.2
  match_variance: true

forces:
  enabled: true
  type: "morse"
  params:
    Ca: 0.8
    Cr: 0.3
    la: 1.5
    lr: 0.5
    mu_t: 0.3
    rcut_factor: 5.0

alignment:
  enabled: true

# ============================================================================
# DENSITY
# ============================================================================
density:
  nx: 48
  ny: 48
  bandwidth: 4.0

outputs:
  density_resolution: 48
  density_bandwidth: 4.0

# ============================================================================
# TRAINING ICs — 1000 Gaussian runs
# ============================================================================
# Dense coverage: 6×6 position grid × 5 variances × ~6 samples ≈ 1000
# ============================================================================
train_ic:
  type: "mixed_comprehensive"

  gaussian:
    enabled: true
    n_runs: 1000
    positions_x: [1.5, 4.0, 6.5, 8.5, 11.0, 13.5]
    positions_y: [1.5, 4.0, 6.5, 8.5, 11.0, 13.5]
    variances: [0.3, 0.8, 1.5, 2.5, 4.0]
    n_samples_per_config: 6

  uniform:
    enabled: false

  two_clusters:
    enabled: false

  ring:
    enabled: false

# ============================================================================
# TEST ICs — 100 Gaussian runs (serious test set)
# ============================================================================
test_ic:
  type: "mixed_test_comprehensive"

  gaussian:
    enabled: true
    n_runs: 100
    test_positions_x: [3.0, 5.5, 7.5, 9.5, 12.0]
    test_positions_y: [3.0, 5.5, 7.5, 9.5, 12.0]
    test_variances: [0.5, 1.2, 2.0, 3.2]
    n_samples_per_config: 1
    extrapolation_positions: [[0.5, 0.5], [14.5, 0.5], [0.5, 14.5], [14.5, 14.5], [7.5, 0.5], [0.5, 7.5], [14.5, 7.5], [7.5, 14.5]]
    extrapolation_variance: [0.5, 3.2]

  uniform:
    enabled: false

  two_clusters:
    enabled: false

  ring:
    enabled: false

# ============================================================================
# ROM — Maximum power, no guardrails
# ============================================================================
rom:
  subsample: 3           # 1500 steps / 3 = 500 frames per IC

  # 98% energy → auto-select modes (expect d≈35)
  pod_energy: 0.98

  # NO eigenvalue_threshold — raw MVAR

  models:
    mvar:
      enabled: true
      lag: 7              # Deep temporal memory (0.84s)
      ridge_alpha: 1.0e-4  # Light regularization — 493K samples
      # NO eigenvalue_threshold

    lstm:
      enabled: true
      lag: 7              # Match MVAR lag
      hidden_units: 256   # MASSIVE capacity
      num_layers: 4       # Deep network
      max_epochs: 10000   # Extreme training
      patience: 500       # Very patient — let it converge
      batch_size: 512     # Large batch for stability with 493K samples
      learning_rate: 1.0e-3
      weight_decay: 1.0e-5
      gradient_clip: 1.0
      dropout: 0.15       # Slightly more dropout for deeper net
      scheduler_patience: 100  # LR scheduler patience

# ============================================================================
# EVALUATION — Raw output, no tricks
# ============================================================================
eval:
  metrics: ["r2", "rmse"]
  save_forecasts: true
  save_time_resolved: true
  forecast_start: 0.84     # = lag(7) × subsample(3) × dt(0.04)
  clamp_negative: false    # Raw output
