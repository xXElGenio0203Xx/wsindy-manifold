---
# ============================================================================
# SYNTHESIS V2 — Stability-aware ROM with compact latent space
# ============================================================================
#
# ROOT CAUSE FROM V1 DIAGNOSIS:
#   MVAR companion matrix spectral radius ρ = 1.322 → predictions EXPLODE
#   42/95 eigenvalues outside unit circle
#   Training R² = 0.9994 but latent forecast R² = -83.6
#   19 POD modes × lag 5 = 1,805 params with only 6,512 samples (3.6× ratio)
#   Ridge α = 1e-4 far too small to constrain eigenvalues
#
# FIXES APPLIED:
#
#   [A] ENFORCE STABILITY — eigenvalue_threshold: 0.98
#       Fixed mvar_trainer.py to check FULL companion matrix (not just last lag)
#       Iteratively scales coefficients until ρ ≤ 0.98
#
#   [B] REDUCE MODEL COMPLEXITY — fixed_modes: 8
#       19 modes → 8 modes: params drop from 1,805 to 192 (p=3) or 320 (p=5)
#       Data/param ratio: 10,800/192 = 56× (excellent)
#       Trade-off: POD ceiling drops from 0.88 to ~0.70-0.75
#       BUT with stable MVAR, actual R² should approach the ceiling
#
#   [C] REDUCE LAG — lag: 3 (from 5)
#       Fewer lags = fewer unstable companion modes
#       3 lags × 8² = 192 params (vs 1,805 in v1)
#       Still captures 0.36s of history at Δt_ROM = 0.12s
#
#   [D] INCREASE REGULARIZATION — ridge_alpha: 1.0
#       α=1.0 vs 1e-4: aggressively shrinks coefficients toward zero
#       Combined with stability check = double safety net
#
#   [E] MORE TRAINING DATA — 300 runs (vs 200)
#       More samples → better statistical support
#       10,800 samples for 192 params = 56× ratio (excellent)
#
#   [F] LONGER FORECAST — T=6s, forecast_start=0.36s
#       With stable MVAR we can afford a longer forecast horizon
#       forecast_start = lag × subsample × dt = 3 × 3 × 0.04 = 0.36s
#
# EXPECTED PERFORMANCE:
#   POD ceiling: ~0.70-0.75 (fewer modes)
#   MVAR forecast R²: 0.50-0.70 (stable predictions close to ceiling)
#   Final R² (after lifting): should track MVAR closely since
#     lifting is a linear operation and modes are fewer/smoother
#
# BUDGET:
#   ROM frames/run: 150/3 = 50
#   MVAR samples/run: 50 - 3 = 47 (after removing first p lags) 
#   Total samples: 300 × 47 ≈ 14,100 (conservative; actual may be ~36 for train_frac)
#   MVAR params: 8² × 3 + 8 = 200  (with intercept)
#   Params/data ratio: 200 / 10,800 ≈ 0.02  (massively over-determined!)
#
# ============================================================================

experiment_name: "synthesis_v2"

# ============================================================================
# SIMULATION PARAMETERS (same physics as v1)
# ============================================================================
sim:
  N: 100                   # 100 particles
  T: 6.0                   # Slightly longer — stable MVAR can handle it
  dt: 0.04                 # 150 frames/run
  Lx: 15.0
  Ly: 15.0
  bc: "periodic"

model:
  type: "discrete"
  speed: 1.5
  speed_mode: "constant_with_forces"

params:
  R: 2.5

noise:
  kind: "gaussian"
  eta: 0.2
  match_variance: true

forces:
  enabled: true
  type: "morse"
  params:
    Ca: 0.8
    Cr: 0.3
    la: 1.5
    lr: 0.5
    mu_t: 0.3
    rcut_factor: 5.0

alignment:
  enabled: true

# ============================================================================
# DENSITY FIELD — Same as v1
# ============================================================================
density:
  nx: 48
  ny: 48
  bandwidth: 4.0           # Wide bandwidth → smooth density → fast SVD decay

outputs:
  density_resolution: 48
  density_bandwidth: 4.0

# ============================================================================
# TRAINING: 300 runs (up from 200) for more data
# ============================================================================
train_ic:
  type: "mixed_comprehensive"

  gaussian:
    enabled: true
    n_runs: 180             # 60% of 300
    positions_x: [3.0, 6.0, 9.0, 12.0]
    positions_y: [3.0, 6.0, 9.0, 12.0]
    variances: [0.8, 1.5, 2.5]
    n_samples_per_config: 3        # 4×4×3×3 = 144, pad to 180

  uniform:
    enabled: true
    n_runs: 75              # 25% of 300
    n_samples: 75

  two_clusters:
    enabled: true
    n_runs: 45              # 15% of 300
    separations: [3.0, 5.0, 7.0]
    sigmas: [1.0, 2.0]
    n_samples_per_config: 7        # 3×2×7 = 42, pad to 45

  ring:
    enabled: false

# ============================================================================
# TEST: 30 runs (same structure as v1)
# ============================================================================
test_ic:
  type: "mixed_test_comprehensive"

  gaussian:
    enabled: true
    n_runs: 15
    test_positions_x: [4.5, 7.5, 10.5]
    test_positions_y: [4.5, 7.5, 10.5]
    test_variances: [1.2]
    n_samples_per_config: 1
    extrapolation_positions: [[2.0, 2.0], [13.0, 2.0], [2.0, 13.0], [13.0, 13.0], [7.5, 1.5], [1.5, 7.5]]
    extrapolation_variance: [1.2]

  uniform:
    enabled: true
    n_runs: 8

  two_clusters:
    enabled: true
    n_runs: 7
    test_separations: [4.0, 6.0]
    test_sigmas: [1.5]
    n_samples_per_config: 3
    extrapolation_separations: [2.0]
    extrapolation_sigma: [1.5]

  ring:
    enabled: false

# ============================================================================
# ROM CONFIGURATION — STABILITY-AWARE
# ============================================================================
rom:
  subsample: 3             # Δt_ROM = 0.12s (same as v1)

  # CRITICAL CHANGE: fixed 8 modes instead of energy threshold
  # 19 modes was too many — params grew to 1,805 and MVAR went unstable
  # 8 modes: params = 8²×3 = 192 (96% fewer), well over-determined
  fixed_modes: 8

  # STABILITY ENFORCEMENT — the key fix
  # mvar_trainer.py now builds FULL companion matrix and checks spectral radius
  # If ρ > 0.98, iteratively scales coefficients until stable
  eigenvalue_threshold: 0.98

  models:
    mvar:
      enabled: true
      lag: 3               # Reduced from 5: fewer params, fewer unstable modes
                            # 3 ROM steps = 0.36s of history (still sufficient)
      ridge_alpha: 1.0     # 10,000× stronger than v1's 1e-4
                            # Aggressively shrinks eigenvalues toward zero
                            # Combined with eigenvalue_threshold = double safety

      # Also look for eigenvalue_threshold in mvar config block
      eigenvalue_threshold: 0.98

    lstm:
      enabled: true
      lag: 3               # Match MVAR lag for fair comparison
      hidden_units: 32     # Reduced from 64: d=8 doesn't need large hidden layer
                            # Total params: 4(32×8 + 32² + 32×2 + 8) ≈ 5,500
                            # vs 53k in v1 — much better ratio
      num_layers: 2
      max_epochs: 2000     # More epochs since model is smaller
      patience: 80         # More patience for convergence
      batch_size: 256
      learning_rate: 3.0e-4  # Slightly lower for stability
      weight_decay: 1.0e-3
      gradient_clip: 1.0
      dropout: 0.15        # Slightly more dropout for generalization

# ============================================================================
# EVALUATION
# ============================================================================
eval:
  metrics: ["r2", "rmse"]
  save_forecasts: true
  save_time_resolved: true
  forecast_start: 0.36     # = lag(3) × subsample(3) × dt(0.04) = 0.36s
                            # Condition on exactly 3 ROM frames
                            # Forecast: 50 - 3 = 47 ROM frames = 5.64s
