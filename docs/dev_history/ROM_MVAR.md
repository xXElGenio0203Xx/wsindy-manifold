# ROM/MVAR Pipeline Guide

Complete guide for Reduced-Order Models (ROM) using global POD + MVAR for collective motion dynamics. Covers training, evaluation, visualization, and deployment on Oscar HPC.

---

## Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Pipeline Stages](#pipeline-stages)
4. [Configuration](#configuration)
5. [Training Details](#training-details)
6. [Evaluation Metrics](#evaluation-metrics)
7. [Visualization](#visualization)
8. [Oscar Deployment](#oscar-deployment)
9. [Troubleshooting](#troubleshooting)
10. [Quick Reference](#quick-reference)

---

## Overview

**EF-ROM** (Empirical Flow Reduced-Order Model) implements the evaluation strategy from Alvarez et al. for forecasting collective motion dynamics using low-dimensional representations.

### Pipeline Overview

```
Ensemble Generation → Global POD Basis → MVAR Training → Evaluation → Visualization
```

**Three-phase workflow:**

1. **Phase 1 - Ensemble Generation**: Generate multiple simulations with varied initial conditions
2. **Phase 2 - Global POD Basis**: Compute low-dimensional spatial modes across all runs
3. **Phase 3 - MVAR Training**: Train linear autoregressive model in latent space

**Key Features:**
- **Global POD**: Single basis across entire ensemble
- **MVAR Model**: Multivariate AutoRegressive forecasting in latent space
- **Clean separation**: Computation (Oscar-friendly) vs visualization (local)
- **Comprehensive metrics**: R², RMSE, mass conservation, per-run breakdown

### Output Directory Structure

```
rom_mvar/
└── <experiment_name>/
    ├── model/
    │   ├── pod_basis.npz           # POD modes, mean mode, grid info
    │   ├── mvar_params.npz         # A_0, A_j matrices, order, latent_dim
    │   └── train_summary.json      # Training metadata
    ├── test_ics/
    │   ├── ic_0000/
    │   │   ├── true_density.npz
    │   │   ├── pred_density.npz
    │   │   ├── metrics_timeseries.csv
    │   │   ├── metrics_summary.json
    │   │   └── videos/             # Generated by visualize script
    │   │       ├── density_comparison.gif
    │   │       └── error_dashboard.png
    │   └── ic_0001/
    │       └── ...
    └── aggregate_metrics/
        ├── all_test_metrics.csv
        └── aggregate_summary.json
```

---

## Quick Start

### Local Workflow (One Command Per Stage)

```bash
# Stage 1: Generate ensemble
rectsim ensemble --config configs/rom_test.yaml

# Stage 2: Build POD basis
python scripts/rom_build_pod.py \
  --experiment_name my_exp \
  --sim_root simulations/MODEL_ID/runs \
  --train_runs 0 1 2 3 4 5 6 7 \
  --test_runs 8 9

# Stage 3: Train MVAR
python scripts/rom_train_mvar.py \
  --experiment_name my_exp \
  --mvar_order 4 \
  --ridge 1e-6

# Stage 4: Evaluate
python scripts/rom_evaluate.py \
  --experiment_name my_exp \
  --sim_root simulations/MODEL_ID/runs \
  --no_videos  # Optional: faster without videos
```

### Oscar HPC (Automated Pipeline)

```bash
# Training on Oscar
sbatch scripts/slurm/job_mvar_train.sh configs/rom_mvar_example.yaml

# Or use complete pipeline script
bash scripts/slurm/submit_mvar_pipeline.sh configs/rom_mvar_example.yaml

# Evaluation (runs automatically if using pipeline script)
sbatch scripts/slurm/job_mvar_eval.sh gentle_clustering configs/rom_mvar_example.yaml
```

### Visualization (Local After Downloading Results)

```bash
# Sync results from Oscar
rsync -avz oscar:/users/emaciaso/src/wsindy-manifold/rom_mvar/gentle_clustering/ \
    rom_mvar/gentle_clustering/

# Generate videos and plots
python scripts/rom_mvar_visualize.py --experiment gentle_clustering
```

---

## Pipeline Stages

### Stage 1: Ensemble Generation

Generate multiple simulation runs with varied initial conditions.

**Required config sections:**
```yaml
model: social_force
sim:
  N: 100
  Lx: 20.0
  Ly: 20.0
params:
  dt: 0.01
  T_max: 50.0
  save_every: 1
ensemble:
  n_runs: 10
  ic_variation:
    method: "uniform"  # uniform, gaussian, grid
    params:
      x_range: [0, 20]
      y_range: [0, 20]
```

**Output:**
```
simulations/social_force_N100_T500.../
├── run_0001/
│   ├── density.npz      # (T, ny, nx) density movie
│   ├── run.json         # Metadata
│   └── ...
├── run_0002/
│   └── ...
```

### Stage 2: Global POD Basis

Compute low-dimensional spatial modes across all training runs.

**Function:** `compute_pod(X, r=None, energy_threshold=0.995)`

**Steps:**
1. Load density movies from all training runs
2. Flatten: `(T_r, ny, nx)` → `(T_r, d)` where `d = ny * nx`
3. Concatenate: All runs → `(T_total, d)`
4. Center: Subtract global spatial mean
5. SVD: `X = U @ diag(S) @ Vt`
6. Energy: `E_k = sum(S[:k]**2) / sum(S**2)`
7. Mode selection: Choose `r` such that `E_r >= energy_threshold`

**Output (`pod/basis.npz`):**
- `Phi`: (d, r) POD modes
- `S`: Singular values
- `mean`: (d,) Global mean
- `energy`: Cumulative energy fractions
- `r`: Number of modes retained

### Stage 3: MVAR Training

Train Multivariate AutoRegressive model on latent trajectories.

**MVAR Model:**
```
a_{t} = A_0 + A_1 a_{t-1} + A_2 a_{t-2} + ... + A_p a_{t-p} + ε_t
```

Where:
- `a_t`: (r,) latent coordinates at time t
- `A_j`: (r, r) coefficient matrices
- `A_0`: (r,) intercept
- `p`: MVAR order (memory depth)

**Fitting:**
- Ridge regression with regularization parameter λ
- Training data: First 80% of each trajectory
- Validation: Remaining 20%

**Output (`mvar/mvar_model.npz`):**
- `A_coefs`: [(r, r)] list of length p
- `A_0`: (r,) intercept
- `order`: int
- `ridge`: float

### Stage 4: Evaluation

Test generalization on unseen initial conditions.

**Metrics computed:**
- **R² Score**: Coefficient of determination (1.0 = perfect)
- **RMSE**: Root mean squared error in density field
- **Mass Conservation**: Total mass preserved per timestep
- **Per-timestep errors**: Track degradation over forecast horizon

**Output:**
- `metrics_summary.json`: Per-run aggregate metrics
- `metrics_timeseries.csv`: Per-timestep breakdown
- `aggregate_metrics.json`: Mean/std across all test runs

---

## Configuration

### Example Config (`configs/rom_mvar_example.yaml`)

```yaml
# Base simulation parameters
model: social_force
sim:
  N: 100
  Lx: 20.0
  Ly: 20.0
  nx: 64
  ny: 64
params:
  dt: 0.01
  T_max: 50.0
  save_every: 1
forces:
  repulsion:
    C_rep: 5.0
    l_rep: 1.0
  attraction:
    C_att: 1.0
    l_att: 3.0
  alignment:
    kappa: 0.5
    r_align: 3.0
noise:
  kind: gaussian
  sigma_v: 0.1
ensemble:
  n_runs: 30
  train_frac: 0.7
  ic_variation:
    method: uniform
    params:
      x_range: [2, 18]
      y_range: [2, 18]
      v_max: 0.5
mvar:
  order: 4
  ridge: 1e-6
  energy_threshold: 0.995
```

### Key Parameters

| Parameter | Description | Typical Range |
|-----------|-------------|---------------|
| `mvar.order` | MVAR memory depth (# lags) | 2-8 |
| `mvar.ridge` | Ridge regularization strength | 1e-7 to 1e-4 |
| `mvar.energy_threshold` | POD energy retention | 0.95-0.999 |
| `ensemble.n_runs` | Total simulation runs | 20-100 |
| `ensemble.train_frac` | Fraction for training | 0.6-0.8 |
| `sim.nx, sim.ny` | Grid resolution | 32-128 |

---

## Training Details

### Training Strategies

**Three training approaches** (from `MVAR_TRAINING_STRATEGY.md`):

1. **Global Training (Recommended)**
   - Single POD basis across all runs
   - One MVAR model for all trajectories
   - Best for ensemble forecasting
   - **Use when**: Testing generalization to new ICs

2. **Per-Run Training**
   - Separate POD basis per run
   - Separate MVAR model per run
   - Best for single-trajectory accuracy
   - **Use when**: Deep dive into specific dynamics

3. **Hybrid Training**
   - Global POD basis
   - Per-run MVAR models
   - Compromise between approaches

### Recommended Training Parameters

**For quick tests:**
```yaml
ensemble.n_runs: 10
mvar.order: 2
mvar.ridge: 1e-6
mvar.energy_threshold: 0.95
```

**For publication-quality results:**
```yaml
ensemble.n_runs: 50
mvar.order: 6
mvar.ridge: 1e-7
mvar.energy_threshold: 0.998
```

**For Oscar HPC (large-scale):**
```yaml
ensemble.n_runs: 100
mvar.order: 8
mvar.ridge: 1e-6
mvar.energy_threshold: 0.999
sim.nx: 128
sim.ny: 128
```

---

## Evaluation Metrics

### Key Metrics

**R² Score:**
```
R² = 1 - (Σ(y_true - y_pred)² / Σ(y_true - mean(y_true))²)
```
- **Range**: (-∞, 1.0]
- **Interpretation**: 1.0 = perfect, 0.0 = baseline, < 0 = worse than mean
- **Target**: > 0.85 for good generalization

**RMSE:**
```
RMSE = sqrt(mean((ρ_true - ρ_pred)²))
```
- **Units**: Same as density field
- **Lower is better**

**Mass Conservation:**
```
total_mass(t) = Σ(ρ(x,y,t) * dx * dy)
```
- **Target**: < 1% deviation from initial mass

### Aggregate Metrics Output

```json
{
  "mean_r2": 0.893,
  "std_r2": 0.047,
  "mean_rmse": 0.0123,
  "std_rmse": 0.0034,
  "all_mass_ok": true,
  "n_test_runs": 9,
  "timestamp": "2025-01-20T14:30:00"
}
```

### Checking Results

```bash
# Aggregate performance
cat rom_mvar/my_exp/aggregate_metrics/aggregate_summary.json | grep -E "mean_r2|mean_rmse|all_mass_ok"

# Per-run breakdown
for f in rom_mvar/my_exp/test_ics/ic_*/metrics_summary.json; do
  echo "$f: R²=$(jq .r2_mean $f)"
done
```

---

## Visualization

### Generate Videos (Local Only)

```bash
# After downloading results from Oscar
python scripts/rom_mvar_visualize.py --experiment gentle_clustering

# Output: videos/ directory created in each test_ics/ic_XXXX/
```

**Generated artifacts:**
- `density_comparison.gif`: Side-by-side true vs predicted
- `error_dashboard.png`: 4-panel diagnostic (error map, RMSE time series, R² time series, mass conservation)

### Visualization Options

```bash
# Skip video generation (faster)
python scripts/rom_mvar_visualize.py --experiment my_exp --no_videos

# Custom output directory
python scripts/rom_mvar_visualize.py --experiment my_exp --out_dir custom_outputs/

# Process specific test ICs
python scripts/rom_mvar_visualize.py --experiment my_exp --test_ics 0 1 2
```

---

## Oscar Deployment

See [OSCAR.md](OSCAR.md) for complete Oscar HPC guide.

### Quick Oscar Workflow

```bash
# 1. Connect
ssh oscar

# 2. Activate environment
cd /users/emaciaso/src/wsindy-manifold
source setup_oscar_env.sh

# 3. Submit training job
sbatch scripts/slurm/job_mvar_train.sh configs/rom_mvar_example.yaml

# 4. Monitor
squeue -u $USER
tail -f slurm_logs/mvar_train_JOBID.out

# 5. Submit evaluation (after training completes)
sbatch scripts/slurm/job_mvar_eval.sh gentle_clustering configs/rom_mvar_example.yaml
```

### Automated Pipeline Script

```bash
# Single command for complete pipeline
bash scripts/slurm/submit_mvar_pipeline.sh configs/rom_mvar_example.yaml

# This submits:
# 1. Ensemble generation (depends on: none)
# 2. MVAR training (depends on: ensemble)
# 3. MVAR evaluation (depends on: training)
```

### Downloading Results

```bash
# From your local machine
rsync -avz oscar:/users/emaciaso/src/wsindy-manifold/rom_mvar/gentle_clustering/ \
    rom_mvar/gentle_clustering/

# Then visualize locally
python scripts/rom_mvar_visualize.py --experiment gentle_clustering
```

---

## Troubleshooting

### Common Issues

| Issue | Symptoms | Fix |
|-------|----------|-----|
| **Low R² score** | R² < 0.7 | Increase `energy_threshold` (0.998) or `mvar_order` (6-8) |
| **NaN forecasts** | Predicted density contains NaN | Increase `ridge` (1e-5) or decrease `mvar_order` |
| **Out of memory** | Job killed on Oscar | Reduce `nx, ny` (64→32) or use `--no_videos` |
| **Mass not conserved** | `all_mass_ok: false` | Expected with truncated POD, use more modes (0.999 threshold) |
| **Slow training** | Job times out | Reduce `n_runs` or `nx, ny`, or request more time |
| **Missing density.npz** | FileNotFoundError | Ensure `save_density=true` in config |

### Debugging Tips

**Check POD energy:**
```python
import numpy as np
basis = np.load('rom_mvar/my_exp/model/pod_basis.npz')
print(f"Modes retained: {basis['Phi'].shape[1]}")
print(f"Energy captured: {basis['energy'][-1]:.4f}")
```

**Inspect MVAR coefficients:**
```python
mvar = np.load('rom_mvar/my_exp/model/mvar_params.npz')
print(f"MVAR order: {mvar['order']}")
print(f"Latent dim: {mvar['latent_dim']}")
print(f"A_1 norm: {np.linalg.norm(mvar['A_1']):.4f}")
```

**Check test trajectories:**
```bash
ls rom_mvar/my_exp/test_ics/
# Should see: ic_0000/ ic_0001/ ... ic_00XX/
```

---

## Quick Reference

### Complete Workflow Commands

```bash
# STAGE 1: Ensemble
rectsim ensemble --config configs/rom_test.yaml

# STAGE 2: POD
python scripts/rom_build_pod.py \
  --experiment_name my_exp \
  --sim_root simulations/MODEL_ID/runs \
  --train_runs 0 1 2 3 4 5 6 7 \
  --test_runs 8 9

# STAGE 3: MVAR
python scripts/rom_train_mvar.py \
  --experiment_name my_exp \
  --mvar_order 4 \
  --ridge 1e-6

# STAGE 4: Evaluate
python scripts/rom_evaluate.py \
  --experiment_name my_exp \
  --sim_root simulations/MODEL_ID/runs \
  --no_videos
```

### Oscar One-Liner

```bash
bash scripts/slurm/submit_mvar_pipeline.sh configs/rom_mvar_example.yaml
```

### Output Locations

```
rom_mvar/my_exp/
├── model/
│   ├── pod_basis.npz
│   ├── mvar_params.npz
│   └── train_summary.json
├── test_ics/
│   ├── ic_0000/
│   │   ├── true_density.npz
│   │   ├── pred_density.npz
│   │   ├── metrics_summary.json
│   │   └── videos/
│   └── ...
└── aggregate_metrics/
    └── aggregate_summary.json
```

### Parameter Tuning Cheat Sheet

```bash
# More accuracy
--energy_threshold 0.998
--mvar_order 8

# Faster training
--energy_threshold 0.95
--mvar_order 2

# More stability
--ridge 1e-5

# More training data
--train_frac 0.9
```

---

## File Formats Reference

### `pod_basis.npz`
- `Phi`: (d, r) POD spatial modes
- `S`: (min(T_total, d),) Singular values
- `mean`: (d,) Global spatial mean
- `energy`: (r,) Cumulative energy fractions
- `r`: int, number of modes retained
- `grid_shape`: (ny, nx)

### `mvar_params.npz`
- `A_0`: (r,) Intercept vector
- `A_1`, `A_2`, ..., `A_p`: (r, r) Coefficient matrices
- `order`: int, MVAR lag order
- `ridge`: float, regularization parameter
- `latent_dim`: int (same as r)

### `metrics_summary.json`
```json
{
  "r2_mean": 0.89,
  "rmse_mean": 0.012,
  "mass_conserved": true,
  "forecast_horizon": 500,
  "n_timesteps": 500
}
```

### `aggregate_summary.json`
```json
{
  "mean_r2": 0.893,
  "std_r2": 0.047,
  "mean_rmse": 0.0123,
  "std_rmse": 0.0034,
  "all_mass_ok": true,
  "n_test_runs": 9
}
```

---

## Additional Resources

- **Oscar Guide**: [OSCAR.md](OSCAR.md) - Complete Oscar HPC workflow
- **Development**: [DEVELOPMENT.md](DEVELOPMENT.md) - Recent patches and known issues
- **Ensemble Guide**: [ENSEMBLE.md](ENSEMBLE.md) - Ensemble generation details
- **Main README**: [README.md](README.md) - Package overview and installation

**Archived documentation** (historical reference): `docs/archive/`
