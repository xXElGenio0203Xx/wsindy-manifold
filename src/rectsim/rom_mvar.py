"""ROM/MVAR evaluation pipeline for collective motion dynamics.

This module implements a clean separation between:
1. Training: Global POD + MVAR on augmented ensemble (varied ICs, same dynamics)
2. Evaluation: Test generalization on unseen ICs with comprehensive metrics
3. Visualization: Post-hoc generation of plots and videos from saved data

Directory structure:
    rom_mvar/
    └── <experiment_name>/
        ├── model/
        │   ├── pod_basis.npz           # POD modes, mean mode, grid info
        │   ├── mvar_params.npz         # A_0, A_j matrices, order, latent_dim
        │   └── train_summary.json      # Training metadata
        ├── test_ics/
        │   ├── ic_0000/
        │   │   ├── true_density.npz
        │   │   ├── pred_density.npz
        │   │   ├── metrics_timeseries.csv
        │   │   ├── metrics_summary.json
        │   │   └── videos/             # Optional (generated by visualize)
        │   │       ├── density_true.mp4
        │   │       ├── density_pred.mp4
        │   │       └── comparison.mp4
        │   └── ic_0001/
        │       └── ...
        └── aggregate_metrics/
            ├── all_test_metrics.csv
            └── aggregate_summary.json

Author: Maria
Date: November 2025
"""

from __future__ import annotations

import json
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
from scipy.linalg import lstsq


# ============================================================================
# Configuration
# ============================================================================


@dataclass
class ROMTrainConfig:
    """Configuration for ROM training (POD + MVAR).

    Attributes
    ----------
    experiment_name : str
        Unique identifier for this experiment.
    num_train_ics : int
        Number of training ICs with varied initial conditions.
    train_seeds : list of int or None
        Explicit seeds for training ICs. If None, uses range(num_train_ics).
    mvar_order : int
        MVAR model order (number of lags).
    ridge : float
        Ridge regularization parameter for MVAR.
    energy_threshold : float
        POD energy threshold for mode selection (0.995 = 99.5% energy).
    latent_dim : int or None
        Fixed number of POD modes (overrides energy_threshold if set).
    train_frac : float
        Fraction of time steps used for MVAR training (1.0 = use all).
    rom_root : Path
        Root directory for ROM outputs (rom_mvar/).
    """

    experiment_name: str
    num_train_ics: int = 10
    train_seeds: List[int] | None = None
    mvar_order: int = 4
    ridge: float = 1e-6
    energy_threshold: float = 0.995
    latent_dim: int | None = None
    train_frac: float = 1.0
    rom_root: Path = field(default_factory=lambda: Path("rom_mvar"))

    def __post_init__(self):
        if self.train_seeds is None:
            self.train_seeds = list(range(self.num_train_ics))
        elif len(self.train_seeds) != self.num_train_ics:
            raise ValueError(
                f"train_seeds length ({len(self.train_seeds)}) must match "
                f"num_train_ics ({self.num_train_ics})"
            )

    def to_dict(self) -> Dict:
        """Convert to JSON-serializable dict."""
        d = asdict(self)
        d["rom_root"] = str(self.rom_root)
        return d


@dataclass
class ROMEvalConfig:
    """Configuration for ROM evaluation on unseen ICs.

    Attributes
    ----------
    experiment_name : str
        Name of trained ROM experiment to evaluate.
    num_test_ics : int
        Number of unseen test ICs to evaluate.
    test_seeds : list of int or None
        Explicit seeds for test ICs. If None, uses range(1000, 1000+num_test_ics).
    forecast_horizon : int or None
        Number of time steps to forecast. If None, forecasts to end of simulation.
    error_tolerance : float
        Tolerance threshold for computing τ (tolerance horizon).
    error_metric : str
        Metric to use for τ ('rmse', 'r2', 'e2', 'einf', 'mass_error').
    generate_videos : bool
        Whether to generate videos during evaluation (False for Oscar).
    generate_plots : bool
        Whether to generate plots during evaluation (False for Oscar).
    rom_root : Path
        Root directory for ROM outputs (rom_mvar/).
    """

    experiment_name: str
    num_test_ics: int = 1
    test_seeds: List[int] | None = None
    forecast_horizon: int | None = None
    error_tolerance: float = 0.1
    error_metric: str = "rmse"
    generate_videos: bool = False
    generate_plots: bool = False
    rom_root: Path = field(default_factory=lambda: Path("rom_mvar"))

    def __post_init__(self):
        if self.test_seeds is None:
            # Use non-overlapping seeds starting at 1000
            self.test_seeds = list(range(1000, 1000 + self.num_test_ics))
        elif len(self.test_seeds) != self.num_test_ics:
            raise ValueError(
                f"test_seeds length ({len(self.test_seeds)}) must match "
                f"num_test_ics ({self.num_test_ics})"
            )
        
        valid_metrics = {"rmse", "r2", "e2", "einf", "mass_error"}
        if self.error_metric not in valid_metrics:
            raise ValueError(
                f"error_metric must be one of {valid_metrics}, got {self.error_metric}"
            )

    def to_dict(self) -> Dict:
        """Convert to JSON-serializable dict."""
        d = asdict(self)
        d["rom_root"] = str(self.rom_root)
        return d


# ============================================================================
# Directory structure helpers
# ============================================================================


def setup_rom_directories(experiment_name: str, rom_root: Path) -> Dict[str, Path]:
    """Create standardized ROM directory structure.

    Returns
    -------
    dict
        Paths to key directories: 'base', 'model', 'test_ics', 'aggregate_metrics'.
    """
    base_dir = rom_root / experiment_name
    model_dir = base_dir / "model"
    test_ics_dir = base_dir / "test_ics"
    aggregate_dir = base_dir / "aggregate_metrics"

    model_dir.mkdir(parents=True, exist_ok=True)
    test_ics_dir.mkdir(parents=True, exist_ok=True)
    aggregate_dir.mkdir(parents=True, exist_ok=True)

    return {
        "base": base_dir,
        "model": model_dir,
        "test_ics": test_ics_dir,
        "aggregate_metrics": aggregate_dir,
    }


def setup_test_ic_directory(test_ics_dir: Path, ic_id: int) -> Path:
    """Create directory for a single test IC.

    Parameters
    ----------
    test_ics_dir : Path
        Base test_ics/ directory.
    ic_id : int
        IC index (0-indexed).

    Returns
    -------
    Path
        Directory for this IC (test_ics/ic_XXXX/).
    """
    ic_dir = test_ics_dir / f"ic_{ic_id:04d}"
    ic_dir.mkdir(parents=True, exist_ok=True)
    return ic_dir


# ============================================================================
# POD utilities
# ============================================================================


def compute_global_pod(
    density_snapshots: np.ndarray,
    energy_threshold: float = 0.995,
    latent_dim: int | None = None,
) -> Dict[str, np.ndarray]:
    """Compute global POD basis from stacked density snapshots.

    Parameters
    ----------
    density_snapshots : ndarray, shape (num_snapshots, nx * ny)
        Flattened density snapshots from all training runs.
    energy_threshold : float
        Cumulative energy threshold for selecting modes (default 0.995).
    latent_dim : int or None
        Fixed number of modes. If set, overrides energy_threshold.

    Returns
    -------
    dict
        - 'mean_mode': mean density across all snapshots
        - 'pod_modes': POD basis vectors (modes, spatial_dim)
        - 'singular_values': singular values
        - 'energy': cumulative energy captured by modes
        - 'latent_dim': number of modes retained
    """
    # Extract mean mode
    mean_mode = density_snapshots.mean(axis=0)
    
    # Center data
    X_centered = density_snapshots - mean_mode[None, :]
    
    # SVD (compute thin SVD for efficiency)
    U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
    
    # Cumulative energy
    energy = np.cumsum(s**2) / np.sum(s**2)
    
    # Determine number of modes
    if latent_dim is not None:
        r = min(latent_dim, len(s))
    else:
        r = np.searchsorted(energy, energy_threshold) + 1
        r = min(r, len(s))
    
    # POD modes are rows of Vt (right singular vectors)
    pod_modes = Vt[:r, :]
    
    return {
        "mean_mode": mean_mode,
        "pod_modes": pod_modes,
        "singular_values": s,
        "energy": energy,
        "latent_dim": r,
    }


def project_to_pod(density: np.ndarray, mean_mode: np.ndarray, pod_modes: np.ndarray) -> np.ndarray:
    """Project density field(s) onto POD basis.

    Parameters
    ----------
    density : ndarray, shape (..., nx * ny)
        Density field(s) to project.
    mean_mode : ndarray, shape (nx * ny,)
        Mean mode for centering.
    pod_modes : ndarray, shape (latent_dim, nx * ny)
        POD basis vectors.

    Returns
    -------
    ndarray, shape (..., latent_dim)
        Latent coordinates.
    """
    density_centered = density - mean_mode
    return density_centered @ pod_modes.T


def reconstruct_from_pod(
    latent: np.ndarray,
    mean_mode: np.ndarray,
    pod_modes: np.ndarray,
) -> np.ndarray:
    """Reconstruct density field(s) from POD latent coordinates.

    Parameters
    ----------
    latent : ndarray, shape (..., latent_dim)
        Latent coordinates.
    mean_mode : ndarray, shape (nx * ny,)
        Mean mode for de-centering.
    pod_modes : ndarray, shape (latent_dim, nx * ny)
        POD basis vectors.

    Returns
    -------
    ndarray, shape (..., nx * ny)
        Reconstructed density field(s).
    """
    return latent @ pod_modes + mean_mode


# ============================================================================
# MVAR utilities
# ============================================================================


def fit_mvar(
    latent_trajectories: List[np.ndarray],
    order: int,
    ridge: float = 1e-6,
) -> Dict[str, np.ndarray]:
    """Fit MVAR model on concatenated latent trajectories.

    Model: z_t = A_0 + A_1 z_{t-1} + ... + A_p z_{t-p} + ε_t

    Parameters
    ----------
    latent_trajectories : list of ndarray
        List of latent time series, each shape (T_i, latent_dim).
    order : int
        MVAR order (number of lags).
    ridge : float
        Ridge regularization parameter.

    Returns
    -------
    dict
        - 'A0': intercept vector, shape (latent_dim,)
        - 'A_coeffs': list of coefficient matrices, each (latent_dim, latent_dim)
        - 'order': MVAR order
        - 'latent_dim': latent dimension
    """
    # Concatenate all trajectories
    Z_list = []
    Y_list = []
    
    for Z in latent_trajectories:
        T, d = Z.shape
        for t in range(order, T):
            # Response: z_t
            Y_list.append(Z[t, :])
            # Predictors: [1, z_{t-1}, z_{t-2}, ..., z_{t-p}]
            row = [1.0]
            for lag in range(1, order + 1):
                row.extend(Z[t - lag, :])
            Z_list.append(row)
    
    Z_mat = np.array(Z_list)  # (n_samples, 1 + p * latent_dim)
    Y_mat = np.array(Y_list)  # (n_samples, latent_dim)
    
    # Ridge regression: (Z^T Z + λI)^{-1} Z^T Y
    ZtZ = Z_mat.T @ Z_mat
    ZtY = Z_mat.T @ Y_mat
    reg = ridge * np.eye(ZtZ.shape[0])
    reg[0, 0] = 0  # Don't regularize intercept
    
    # Solve
    coeff_mat, _, _, _ = lstsq(ZtZ + reg, ZtY)
    
    # Extract A_0 and A_j matrices
    d = Y_mat.shape[1]
    A0 = coeff_mat[0, :]
    A_coeffs = []
    for j in range(order):
        start_idx = 1 + j * d
        end_idx = start_idx + d
        A_j = coeff_mat[start_idx:end_idx, :].T
        A_coeffs.append(A_j)
    
    return {
        "A0": A0,
        "A_coeffs": A_coeffs,
        "order": order,
        "latent_dim": d,
    }


def forecast_mvar(
    z_init: np.ndarray,
    A0: np.ndarray,
    A_coeffs: List[np.ndarray],
    horizon: int,
) -> np.ndarray:
    """Forecast latent trajectory using fitted MVAR.

    Parameters
    ----------
    z_init : ndarray, shape (order, latent_dim)
        Initial latent states [z_0, z_1, ..., z_{p-1}].
    A0 : ndarray, shape (latent_dim,)
        MVAR intercept.
    A_coeffs : list of ndarray
        MVAR coefficient matrices [A_1, A_2, ..., A_p].
    horizon : int
        Number of time steps to forecast.

    Returns
    -------
    ndarray, shape (horizon, latent_dim)
        Forecasted latent trajectory.
    """
    p = len(A_coeffs)
    d = A0.shape[0]
    
    if z_init.shape[0] != p:
        raise ValueError(f"z_init must have {p} initial states, got {z_init.shape[0]}")
    
    # Initialize with given states
    z_history = list(z_init)
    z_forecast = []
    
    for _ in range(horizon):
        # z_t = A_0 + A_1 z_{t-1} + ... + A_p z_{t-p}
        z_t = A0.copy()
        for j, A_j in enumerate(A_coeffs):
            z_t += A_j @ z_history[-(j + 1)]
        
        z_forecast.append(z_t)
        z_history.append(z_t)
    
    return np.array(z_forecast)


# ============================================================================
# Metrics
# ============================================================================


def compute_r2(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Coefficient of determination R²."""
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - y_true.mean()) ** 2)
    return 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0


def compute_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Root mean squared error."""
    return np.sqrt(np.mean((y_true - y_pred) ** 2))


def compute_e2(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Relative L2 error."""
    return np.linalg.norm(y_true - y_pred) / (np.linalg.norm(y_true) + 1e-12)


def compute_einf(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Relative L∞ error."""
    return np.max(np.abs(y_true - y_pred)) / (np.max(np.abs(y_true)) + 1e-12)


def compute_mass_error(density_true: np.ndarray, density_pred: np.ndarray, dx: float, dy: float) -> float:
    """Absolute difference in total mass."""
    dA = dx * dy
    mass_true = np.sum(density_true) * dA
    mass_pred = np.sum(density_pred) * dA
    return abs(mass_true - mass_pred)


def compute_tolerance_horizon(
    error_timeseries: np.ndarray,
    tolerance: float,
) -> int:
    """Compute tolerance horizon τ.

    τ = max time such that error(t) <= tolerance for all t <= τ.

    Parameters
    ----------
    error_timeseries : ndarray, shape (T,)
        Error over time.
    tolerance : float
        Tolerance threshold.

    Returns
    -------
    int
        Tolerance horizon (number of time steps within tolerance).
    """
    within_tol = error_timeseries <= tolerance
    if not within_tol[0]:
        return 0
    
    # Find first time where error exceeds tolerance
    violations = np.where(~within_tol)[0]
    if len(violations) == 0:
        return len(error_timeseries)
    else:
        return violations[0]


def compute_timeseries_metrics(
    density_true: np.ndarray,
    density_pred: np.ndarray,
    dx: float,
    dy: float,
) -> pd.DataFrame:
    """Compute per-time-step metrics.

    Parameters
    ----------
    density_true : ndarray, shape (T, nx, ny)
        True density evolution.
    density_pred : ndarray, shape (T, nx, ny)
        Predicted density evolution.
    dx, dy : float
        Grid spacing.

    Returns
    -------
    DataFrame
        Columns: t, r2, rmse, e2, einf, mass_error
    """
    T = density_true.shape[0]
    metrics = []
    
    for t in range(T):
        rho_true = density_true[t].ravel()
        rho_pred = density_pred[t].ravel()
        
        metrics.append({
            "t": t,
            "r2": compute_r2(rho_true, rho_pred),
            "rmse": compute_rmse(rho_true, rho_pred),
            "e2": compute_e2(rho_true, rho_pred),
            "einf": compute_einf(rho_true, rho_pred),
            "mass_error": compute_mass_error(
                density_true[t], density_pred[t], dx, dy
            ),
        })
    
    return pd.DataFrame(metrics)


def compute_summary_metrics(
    metrics_df: pd.DataFrame,
    error_tolerance: float,
    error_metric: str = "rmse",
) -> Dict[str, float]:
    """Compute summary statistics from timeseries metrics.

    Parameters
    ----------
    metrics_df : DataFrame
        Per-time-step metrics.
    error_tolerance : float
        Tolerance for computing τ.
    error_metric : str
        Metric to use for τ computation.

    Returns
    -------
    dict
        Summary statistics.
    """
    tau = compute_tolerance_horizon(
        metrics_df[error_metric].values,
        error_tolerance,
    )
    
    return {
        "r2_mean": float(metrics_df["r2"].mean()),
        "r2_median": float(metrics_df["r2"].median()),
        "r2_min": float(metrics_df["r2"].min()),
        "rmse_mean": float(metrics_df["rmse"].mean()),
        "rmse_median": float(metrics_df["rmse"].median()),
        "rmse_max": float(metrics_df["rmse"].max()),
        "e2_mean": float(metrics_df["e2"].mean()),
        "einf_mean": float(metrics_df["einf"].mean()),
        "mass_error_mean": float(metrics_df["mass_error"].mean()),
        "mass_error_max": float(metrics_df["mass_error"].max()),
        "tolerance_horizon": int(tau),
        "error_metric_for_tau": error_metric,
        "error_tolerance": error_tolerance,
    }


# ============================================================================
# Save/load utilities
# ============================================================================


def save_pod_model(
    model_dir: Path,
    mean_mode: np.ndarray,
    pod_modes: np.ndarray,
    singular_values: np.ndarray,
    energy: np.ndarray,
    grid_info: Dict,
) -> None:
    """Save POD model to disk."""
    np.savez(
        model_dir / "pod_basis.npz",
        mean_mode=mean_mode,
        pod_modes=pod_modes,
        singular_values=singular_values,
        energy=energy,
        latent_dim=pod_modes.shape[0],
        nx=grid_info["nx"],
        ny=grid_info["ny"],
        Lx=grid_info["Lx"],
        Ly=grid_info["Ly"],
    )


def load_pod_model(model_dir: Path) -> Dict:
    """Load POD model from disk."""
    data = np.load(model_dir / "pod_basis.npz")
    return {
        "mean_mode": data["mean_mode"],
        "pod_modes": data["pod_modes"],
        "singular_values": data["singular_values"],
        "energy": data["energy"],
        "latent_dim": int(data["latent_dim"]),
        "grid_info": {
            "nx": int(data["nx"]),
            "ny": int(data["ny"]),
            "Lx": float(data["Lx"]),
            "Ly": float(data["Ly"]),
        },
    }


def save_mvar_model(
    model_dir: Path,
    A0: np.ndarray,
    A_coeffs: List[np.ndarray],
    order: int,
    latent_dim: int,
) -> None:
    """Save MVAR model to disk."""
    # Stack A_coeffs into 3D array for easy storage
    A_stack = np.stack(A_coeffs, axis=0)  # (order, latent_dim, latent_dim)
    
    np.savez(
        model_dir / "mvar_params.npz",
        A0=A0,
        A_coeffs=A_stack,
        order=order,
        latent_dim=latent_dim,
    )


def load_mvar_model(model_dir: Path) -> Dict:
    """Load MVAR model from disk."""
    data = np.load(model_dir / "mvar_params.npz")
    A_stack = data["A_coeffs"]
    A_coeffs = [A_stack[i] for i in range(A_stack.shape[0])]
    
    return {
        "A0": data["A0"],
        "A_coeffs": A_coeffs,
        "order": int(data["order"]),
        "latent_dim": int(data["latent_dim"]),
    }


def save_train_summary(
    model_dir: Path,
    config: ROMTrainConfig,
    pod_info: Dict,
    mvar_info: Dict,
    train_stats: Dict,
) -> None:
    """Save training summary JSON."""
    summary = {
        "config": config.to_dict(),
        "pod": {
            "latent_dim": int(pod_info["latent_dim"]),
            "energy_captured": float(pod_info["energy"][pod_info["latent_dim"] - 1]),
            "compression_ratio": float(
                (pod_info["grid_info"]["nx"] * pod_info["grid_info"]["ny"])
                / pod_info["latent_dim"]
            ),
        },
        "mvar": {
            "order": int(mvar_info["order"]),
            "num_parameters": int(
                mvar_info["latent_dim"] * (1 + mvar_info["order"] * mvar_info["latent_dim"])
            ),
        },
        "training_data": train_stats,
    }
    
    with open(model_dir / "train_summary.json", "w") as f:
        json.dump(summary, f, indent=2)


__all__ = [
    "ROMTrainConfig",
    "ROMEvalConfig",
    "setup_rom_directories",
    "setup_test_ic_directory",
    "compute_global_pod",
    "project_to_pod",
    "reconstruct_from_pod",
    "fit_mvar",
    "forecast_mvar",
    "compute_timeseries_metrics",
    "compute_summary_metrics",
    "save_pod_model",
    "load_pod_model",
    "save_mvar_model",
    "load_mvar_model",
    "save_train_summary",
]
